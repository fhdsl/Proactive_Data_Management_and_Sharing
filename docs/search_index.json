[["index.html", "Proactive Data Management and Sharing About this Course", " Proactive Data Management and Sharing October, 2025 About this Course This course is part of a series of courses for the Informatics Technology for Cancer Research (ITCR) called the Informatics Technology for Cancer Research Education Resource. This material was created by the ITCR Training Network (ITN) which is a collaborative effort of researchers around the United States to support cancer informatics and data science training through resources, technology, and events. This initiative is funded by the following grant: National Cancer Institute (NCI) UE5 CA254170. Our courses feature tools developed by ITCR Investigators and make it easier for principal investigators, scientists, and analysts to integrate cancer informatics into their workflows. Please see our website at www.itcrtraining.org for more information. "],["introduction.html", "Chapter 1 Introduction 1.1 Target Audience 1.2 Topics covered 1.3 Motivation 1.4 Curriculum", " Chapter 1 Introduction 1.1 Target Audience The course is intended for individuals in in the cancer research community who want to learn the best practices and techniques for data management and sharing. This course is written for individuals who: Have or plan to have biomedical data they need to manage Have or plan to apply for NIH funding Work directly with the cancer research data or mentor those who work with data Have not had much training or background in data handling practices 1.2 Topics covered This course covers how to properly manage and share data including: Understand what data sharing is and why is it important? Effectively manage your data including the associated skills relating to data heavy projects Maintain data privacy and comply with data privacy laws Maintain and write effective documentation Keep effective records that will help you track your project properly but securely Create good metadata that can enhance the use your data Organize your project so that it is reproducible and well understood by others Create a data management sharing plan consistent with NIH requirements Identify and use computing resources that best fit your project needs Store and submit your data to repositories 1.3 Motivation The cancer research discipline has evolved into an increasingly complex mix of datasets - research projects are typically cross-disciplinary and contain many types of data in various formats. They often involve multiple collaborators generating data across different sites, with different data standards and infrastructure used to generate the data. Therefore, it is more important than ever to be well-versed in the best practices of data management and sharing. Not only is proper data management and sharing a necessity for cancer research projects to succeed in positively impacting cancer care, but it is now also increasingly a necessity to obtain funding as the NIH and other cancer research funders have implemented mandates that require you to proactively plan to manage and share your data. As a member of the cancer research community, it is imperative that you maintain well-documented metadata and properly share your data. This will benefit you, your colleagues, and the larger community by broadening the reach of your data, enabling data reuse by others, and ultimately accelerating the pace of scientific discovery. This course aims to serve as a starting point to cover the basics of good data management and sharing practices. 1.4 Curriculum How to use this course: This course contains high-level concepts for data management and sharing and can be used as a reference of suggested best practices and associated skills needed for data management and sharing in biomedical research. Keep in mind: Scientific data and research projects come in many different forms, and some content in this course may not apply, especially as the research landscape evolves to adapt and support new technology, methods, and techniques. Therefore, the goal of this course is not to prescribe rigid rules for how to conduct research, but rather serve as a guide to approach data management and sharing in the spirit of the FAIR principles (Findable, Accessible, Interoperable, Reusable). We encourage you to continue to consult with data management experts to suit the needs of your particular project and/or research goals. Disclaimer: This course material is for instructional use only and is not a substitute for legal or ethical advice. The findings and conclusions in this course are those of the authors and do not represent official guidance from the National Institutes of Health. "],["data-sharing-is-important.html", "Chapter 2 Data Sharing is Important", " Chapter 2 Data Sharing is Important Sharing data is critical for optimizing the advancement of scientific understanding. Now that labs all over the world are producing massive amounts of data, there are many discoveries that can be made by simply re-using this existing data. The concept of data re-use is so important that, in January 2023, the NIH began requiring specific practices for data management and sharing. See the announcement here. See this course for more information about how to comply with this policy. Note that many institutes and funding agencies or mechanisms have requirements about how your data can be shared. Typically, data sharing of protected data also requires Institutional Review Board (IRB) approval before the study is conducted. Ensure that you are following those requirements before you share your data. A later section in this course will cover data privacy. There’s so many excellent reasons to put your data in a repository, whether or not a journal requires it: Sharing your data… Makes your project more transparent and thus more likely to be trusted and cited. In fact one study found that articles with links to the data used (in a repository) were cited more than articles without such information or other forms of data sharing (Colavizza et al. 2020). Helps your reduce your own workload so your email inbox isn’t overloaded by requests you probably don’t have time to respond to. Allows others to gain even more insights from your data which shows funders that your data will be used to its maximum potential. Disclaimer: This course material is for instructional use only and is not a substitute for legal or ethical advice. The findings and conclusions in this course are those of the authors and do not represent official guidance from the National Institutes of Health. References "],["defining-reproducibility.html", "Chapter 3 Defining reproducibility 3.1 Learning Objectives 3.2 What is reproducibility 3.3 Reproducibility in daily life 3.4 Reproducibility is worth the effort! 3.5 Reproducibility exists on a continuum!", " Chapter 3 Defining reproducibility 3.1 Learning Objectives 3.2 What is reproducibility There’s been a lot of discussion about what is included in the term reproducibility and there is some discrepancy between fields. For the broad field of cancer research, a reproducible analysis is one that can be re-run by a different researcher and the same result and conclusion is found. Reproducibility is related to repeatability and replicability but it is worth taking time to differentiate these terms Perhaps you are like Ruby and have just found an interesting pattern through your data analysis! This has probably been the result of many months or years on your project and it’s worth celebrating! But before she considers these results a done deal, Ruby should test whether she is able to re-run her own analysis and get the same results again. This is known as repeatability. Given that Ruby’s analysis is repeatable; she may feel confident now to share her preliminary results with her colleague, Avi the Associate. Whether or not someone else will be able to take Ruby’s code and data, re-run the analysis and obtain the same results is known as reproducibility. If Ruby’s results are able to be reproduced by Avi, now Avi may collect new data and use Ruby’s same analysis methods to analyze his data. Whether or not Avi’s new data and results concur with Ruby’s study’s original inferences is known as replicability. You may realize that these levels of research build on each other (like science is supposed to do). In this way, we can think of these in a hierarchy. Skipping any of these levels of research applicability can lead to unreliable results and conclusions. Science progresses when data and hypotheses are put through these levels thoroughly and sequentially. If results are not repeatable, they won’t be reproducible or replicable. Ideally all analyses and results would be reproducible without too much time and effort spent; this would aid in the efficiency of research getting to the next stages and questions. But unfortunately, in practice, reproducibility is not as commonplace as we would hope. Institutions and reward systems generally do not prioritize or even measure reproducibility standards in research and training opportunities for reproducible techniques can be scarce. Reproducible research can often feel like an uphill battle that is made steeper by lack of training opportunities. In this course, we hope to equip your research with the tools you need to enhance the reproducibility of your analyses so this uphill battle is less steep. 3.3 Reproducibility in daily life What does reproducibility mean in the daily life of a researcher? Let’s say Ruby’s results are repeatable in her own hands and she excitedly tells her associate, Avi, about her preliminary findings. Avi is very excited about these results as well as Ruby’s methods! Avi is also interested in Ruby’s analysis methods and results. So Ruby sends Avi the code, data, and methods she used to obtain the results. Now, whether or not Avi is able to obtain the same exact results with this same data and same analysis code will indicate if Ruby’s analysis is reproducible. Ruby may have spent a lot of time on her code and getting it to work on her computer, but whether it will successfully run on Avi’s computer is another story. Often when researchers share their analysis code it leads to a substantial amount of effort on the part of the researcher who has received the code to get it working and this often cannot be done successfully without help from the original code author (Beaulieu-Jones and Greene 2017). This same concept applies to experimental research methods in a laboratory setting. Avi is encountering errors because Ruby’s code was written with Ruby’s computer and local setup in mind and she didn’t know how to make it more generally applicable. Avi is spending a lot of time just trying to re-run Ruby’s same analysis on her same data; he has yet to be able to try the code on any additional data (which will likely bring up even more errors). Imagine a trying to follow an experimental research method in the lab with vague or unclear instructions! Avi is still struggling to work with Ruby’s code and is confused about the goals and approaches the code is taking. After struggling with Avi’s code for an untold amount of time, Avi may decide it’s time to email Ruby to get some clarity. Now both Avi and Ruby are confused about why this analysis isn’t nicely re-running for Avi. Their attempts to communicate about the code through email haven’t helped them clarify anything. Multiple versions of the code may have been sent back and forth between them and now things are taking a lot more time than either of them expected. Perhaps at some point Avi is able to successfully run Ruby’s code on Ruby’s same data. Just because Avi didn’t get any errors doesn’t mean that the code ran exactly the same as it did for Ruby. Lack of errors also doesn’t mean that either Ruby or Avi’s runs of the code ran with high accuracy or that the results can be trusted. Even a small difference in decimal point may indicate a more fundamental difference in how the analysis was performed and this could be due to differences in software versions, settings, or any number of items in their computing environments. This challenge also exists when trying to repeat a research method that you or someone else has written, especially if there aren’t enough details to precisely describe the conditions in which the data was collected. 3.4 Reproducibility is worth the effort! Perhaps you’ve found yourself in a situation like Ruby and Avi; struggling to re-run code or a method that you thought for sure was working a minute ago. In the upcoming chapters, we will discuss how to bolster your projects’ reproducibility. As you apply these reproducible techniques to your own projects, you may feel like it is taking more time to reach endpoints, but keep in mind that reproducible analyses and projects have higher upfront costs but these will absolutely pay off in the long term. Reproducibility in your analyses is not only a time saver for yourself, but also your colleagues, your field, and your future self! You might not change a single character in your code or a step in your method but then return to it in a a few days/months/years and find that it no longer runs! Reproducible code and research methods stands the test of time longer, making ‘future you’ glad you spent the time to work on it. It’s said that your closest collaborator is you from 6 months ago but you don’t reply to email (Broman 2016). Many a researcher has referred to their frustration with their past selves: Dear past-Hadley: PLEASE COMMENT YOUR CODE BETTER. Love present-Hadley — Hadley Wickham (@hadleywickham) April 7, 2016 The more you comment your code, or detail your method and make it clear and readable, your future self will thank you. Reproducible code and research protocols also saves your colleagues time! The more reproducible your methods are, the less time all of your collaborators will need to spend troubleshooting it. The more people who use your methods need to try to troubleshoot it, the more time is wasted. This can add up to a lot of wasted researcher time and effort. But, reproducible code and methods saves everyone exponential amounts of time and effort! It will also motivate individuals to use and cite your methods in the future! 3.5 Reproducibility exists on a continuum! Incremental work on your analyses is good! You do not need to make your analyses perfect on the first try or even within a particular time frame. The first step in creating an analysis is to get it to work once! But the work does not end there. Furthermore, no analysis is or will ever be perfect in that it will not be reproducible in every single context throughout time. incrementally pushing our analyses toward the right of this continuum is the goal. Disclaimer: This course material is for instructional use only and is not a substitute for legal or ethical advice. The findings and conclusions in this course are those of the authors and do not represent official guidance from the National Institutes of Health. References "],["data-privacy.html", "Chapter 4 Data Privacy 4.1 PII (personal identifiable information) 4.2 PHI (protected health information) 4.3 PHI is a subset of PII 4.4 PHI Risk 4.5 Cancer research data and PHI 4.6 How to ensure the privacy of this information 4.7 How is HIPAA enforced?", " Chapter 4 Data Privacy Cancer research often involves personal health data that requires compliance with Health Insurance Portability and Accountability Act (HIPAA) regulations. In this chapter we will discuss data management strategies to maintain compliance with these important regulations. Cancer research often involves the collection of information about research participants that is personal. There are two categories of such information: personal identifiable information (PII) and protected health information (PHI) Note that these are general definitions and whether something counts as PII or PHI has to be evaluated in a case-by-case basis. 4.1 PII (personal identifiable information) PII (personal identifiable information) are aspects of a person that could allow you to identify a person. PII is defined by the US Department of Labor as: “Any representation of information that permits the identity of an individual to whom the information applies to be reasonably inferred by either direct or indirect means.” PII is also defined by the US General Services Administration as: “Information that can be used to distinguish or trace an individual’s identity, either alone or when combined with other personal or identifying information that is linked or linkable to a specific individual.” Why is this term defined by the Department of Labor and the US general Services Administration? Because the Privacy Act of 1974 (privacy_act_2022?), is a US federal law that governs the “collection, maintenance, use and dissemination” of personal information. US agencies have access to a large amount of PII and must act in accordance with the Privacy Act to protect this data. Examples include (but aren’t limited to): Name Telephone number Address Social security number Age Driver’s licenses Medical record numbers Full face photographs IP addresses Some PII as in the examples above can pose significant risk to individuals if other people were to gain access, such as social security numbers. Other PII such as age does not necessarily pose as much risk unless combined with other information. Thus this information is categorized in two ways as being nonsensitive which is easy to find and poses little risk and sensitive information which is harder to find, poses higher risk and requires more protection. 4.1.1 PII Risk What is the risk of PII getting into the hands of people it shouldn’t? Why was the Protection Act necessary? PII can pose a risk for identity theft, which can have financial, professional, criminal, and personal consequences (dinardi_14_2022?), as criminals can get loans and credit card in other people’s names, as well as commit crimes under the guise of other people’s identities. This can result in reputation loss and loss of opportunities. In addition, the leak of PII can also pose a safety risk, as criminals can identify the likely locations of specific individuals if performing targeted crimes. 4.2 PHI (protected health information) The U.S. Department of Health &amp; Human Services describes protected health information (PHI) as: …information including demographic data that relates to: the individual’s past, present or future physical or mental health or condition, the provision of health care to the individual, or the past, present, or future payment for the provision of health care to the individual This includes 18 categories: Patient names Geographical elements (such as a street address, city, county, or zip code) Dates related to the health or identity of individuals (including birthdates, date of admission, date of discharge, date of death, or exact age of a patient older than 89) Telephone numbers Fax numbers Email addresses Social security numbers Medical record numbers Health insurance beneficiary numbers Account numbers Certificate/license numbers Vehicle identifiers Device attributes or serial numbers Digital identifiers, such as website URLs IP addresses Biometric elements, including finger, retinal, and voiceprints Full face photographic images Other identifying numbers or codes 4.3 PHI is a subset of PII PHI is a subset of PII. It is personal identifiable information that relates to or could relate to an individual’s health. Some PII is always PHI, like health insurance numbers or clinical data such as radiology reports with names or other distinguishing features. Other PII becomes PHI based on context. For example, name and email address aren’t necessarily PHI, unless the are in the context of medical care or research. This could be the case if a patient receives notes from the doctor through email or researchers have a database of participants with email addresses that could be used to distinguish the identity of people in the study. 4.4 PHI Risk PHI poses an additional risk rather than just typical PII because it includes sensitive health information. This can be used to determine if an individual has a particular condition or health risk and could be misused in employment or insurance decisions. 4.5 Cancer research data and PHI Certain genomics data, such as whole genome sequencing (essentially a genomic signature), and some radiology images with distinguishing features can be used to identify individuals. Advances in machine learning may further increase the identifiability of these data types in the future. 4.5.1 What genomic data is protected? So what does this mean for the data you handle? A non-comprehensive list of identifiable and protected information: Clinical information in metadata Genomic sequences Whole genome sequences Exome sequencing Whole transcriptome sequencing Single nucleotide polymorphisms Geneology information What is not protected and generally is safe: Summarized cohort data Data in which individuals have been aggregated together is generally safe. For example, a file that includes an average age calculated across all individuals or a large subset would generally be considered safe. However, this may not always be the case with individuals with very rare conditions or individuals belonging to a small group (such as indigenous or pediatric populations). De-identified data Data where all personal identifiers that could link the data to a specific individual are removed, making the data anonymous and safe for sharing under certain conditions. However, in the context of genomic data, de-identification may not always guarantee complete anonymity. This is because genomic data, especially when it contains rare or unique variants, can sometimes be linked back to individuals. The presence of such variants may allow re-identification, particularly if the data is combined with external datasets. As a result, additional protections may be necessary, such as restricted access or data sharing with safeguards in place, to prevent re-identification risks. It has been shown that certain types of de-identified genomic data can be re-identified due to the availability of genomic data in datasets like 23andMe, where relatives with unique genomic features can be used to identify relatives of individuals in studies. The following articles have more extensive information about the current re-identification risk of different genomic data types: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8326502/ https://pubmed.ncbi.nlm.nih.gov/23329047/ https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8411901/ https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5851792/ https://www.eff.org/issues/genetic-information-privacy https://www.eff.org/issues/law-and-medical-privacy https://www.nature.com/articles/d41586-021-00331-5 4.6 How to ensure the privacy of this information Your institution will have guidance on how to protect sensitive data but in general there are 4 main strategies we will summarize here: Limit access to the data The protected data is seen by the smallest number of individuals possible, all of whom have been properly trained and certified to handle the data. Make sure the data is stored in a place that only these few people who are allowed have access to it. If you aren’t sure who has access to a place – don’t put the data there! Aggressively de-identify the shared data Before results or data are shared or published, it must be de-identified. We will discuss more about what this is in the next chapter. If data has been summarized at the cohort-level with no personal identifiers then it is probably safe to share. Consider a data use agreement A Data Use Agreement (DUA) is required even for de-identified data, particularly when human subjects data is shared for research purposes or across institutions. While HIPAA does not mandate a DUA for fully de-identified data under certain conditions, other factors—such as institutional policies, ethical concerns, or specific data sharing agreements—may require one. Data use agreements restrict who can access and use the data that you might share, as well as what they may do with the data. Importantly this needs to be agreed upon by an IRB and consented to by the research participants in some manner before it is in use. See here and here for more information about when you might need a data use agreement. Note that your particular situation and institute may have slightly different rules or restrictions. See here for an example DUA template from the Harvard Catalyst. Be sure to follow the attribution guidelines outlined in the link if you adapt the template for your use. When in doubt, prioritize caution If you are uncertain whether data contains PHI or PII, consult with relevant offices at your institute, such as an IRB, , a research administration office, or a HIPAA compliance office. If you plan to share your data somewhere and you are unsure whether a database or repository is secure and HIPAA compliant, ask those who manage that database or repository to confirm! 4.7 How is HIPAA enforced? The Office for Civil Right (OCR) of the United States Department of Health and Human Services is in charge of enforcing HIPAA compliance. If you feel that someone is using or sharing data that is in violation of HIPAA compliance, in most cases, you should start by attempting to resolve the violation first through local means by contacting research administrators or management. However, you can also choose to file a complaint online using the OCR compliant portal. Note that complaints should be filed within 180 days of the violation. If the OCR determines that a covered entity is in violation (the individuals or institutes who are required to follow HIPAA compliance regulations), then the OCR will follow up to ensure that the entity complies, takes corrective action, or agrees to a settlement. If compliance is not resolved, then the covered entity may have to pay fines. If an individual is not aware of a violation the fine can be quite small, but if it is a repeated issue of willful neglect, they can be fined on the order of $50,000! If the entity committed the violation for malicious reasons for personal gain, they can face much higher fines up to $250,000, and may face jail time of up to 10 years (violations_2018?). 4.7.1 Common Violations Common violations of HIPAA taken from (violations_2018?) are: A lack of encryption If your email or data transfer is intercepted it is important to keep your data safe! This is talked about more in this course Computer hacking or phishing If your computer gets hacked by hackers through a phishing email or otherwise, they could sell the data to third party organizations who could profit off of the information. The data security practices that we will describe in the next chapter will help avoid this. Unauthorized Access Allowing or accidentally allowing fellow lab mates who are not authorized to access the data is a violation of HIPAA. This can lead to other neglectful or malicious practices that result in larger disclosures of PHI. Furthermore, leaving your laptop open to PHI data in public or even at home can pose a risk from people who may walk by. Loss or Theft of Devices If your laptop are external storage device is stolen, data files with PHI can easily be obtained by whoever finds them next. Improper Disposal of data or devices Sometimes there are remnants of your data still on your device! Unsecured access to data Accessing your data from an unsecured WIFI network can also make the data vulnerable. See here for more information about HIPAA and research. "],["why-documentation-is-worth-the-time.html", "Chapter 5 Why Documentation is Worth the Time 5.1 The context of bioinformatics tool development 5.2 Bioinformatics and usability 5.3 Why documentation is worth the time", " Chapter 5 Why Documentation is Worth the Time 5.1 The context of bioinformatics tool development Biomedical software development comes in all shapes and sizes, but many researchers don’t realize the work they are doing is software development. Software developers’ jobs are often defined something like this: Software developers use programming to build software that meets the needs of users. Let’s break this down: Software is often meaning a collection of instructions, data, or computer programs that are used to run machines and carry out particular activities – this can include but isn’t limited to: - Scripts - Workflows - Pipelines - Algorithms and computational methods Scientific software for cancer research includes all of the above items pretty frequently but scientists often don’t think of themselves as software developers. I think this is in part because they don’t picture that they have users. But users can really be anyone! It may start as the person developing the software but may expand to collaborators, random internet strangers, and others in the broader scientific community. The scientific community is full of users! So in fact, many scientific researchers are doing software development everyday! But many don’t have a computer science degree and many of them have never taken a programming class. Self-taught scientific programmers may dismiss themselves as being software developers since they often think of programming as a means to an end – a scientific question may be their main goal. But in the pursuit of that goal they are doing software development along the way! Research, whether code is involved or not, is an exciting but long process – filled with side investigations, tedious troubleshooting, but also ‘Aha’ moments that ultimately can result in an amazing results that you should be proud of! But the code and the methods you use are likely valuable to more than just the singular project you made it for. Indeed, others may have needs for the methods you use and will be excited to come across your code and tools! Other researchers are likely eager to apply your code and methods to their own work but its unfortunately all too common that scientific code is not able to be reused. Even scientists who are skilled with analysis often struggle to make work reproducible. In a large-scale study, only 24% of scientific notebooks ran without errors and only 4.03% produced the same results. There is a great need for reproducible work and a large part of reproducibility is clear and findable documentation! Open source code is a valuable practice for contributing to the scientific community but if the code lacks clear documentation it is incomplete. Undocumented code can lead to a lot of frustration and time inefficiently spent. If a code base’s documentation is non-existent, scarce, out-of-date, or filled with too much jargon, the chances that no one will be able to successfully and efficiently re-use this work, despite their needs to do so. Lack of usability often leads researchers to ditch even the most well-programmed of tools and code. This is the unfortunate and all-too-common result of many bioinformatics code. 5.2 Bioinformatics and usability The lack of emphasis on usability in bioinformatics software development not only hinders progress in cancer research but also undermines the efforts of software developers themselves, who have invested countless hours and significant effort into creating this software. We know that bioinformatics software development doesn’t occur in a vacuum. User experience designers in the field of bioinformatics have commented on reasons why documentation and usability sometimes suffer for bioinformatics tools: Problem 1) Tools developed in academia are often left to deprecate after publication because novelty is often prioritized over long-term maintenance and usability (Mangul et al. 2019). Problem 2) Bioinformatics tool development teams generally don’t have the resources to hire user-centered design experts and the small and specialized user communities are often overbooked and not incentivized to give feedback (Pavelin et al. 2012). Problem 3) There is a lack of resources/education about usability specific to bioinformatics tool developing communities (Pavelin et al. 2012). Unfortunately, this specific course cannot address issues 1 and 2, but will attempt to address problem 3. 5.3 Why documentation is worth the time We realize many software developers feel unenthused about the process of creating documentation or may lack bandwidth to do so. They may know its good for their research, but they just aren’t enthused about it. We’d like to assure you that the effort for creating documentation has a high return payoff for the continued success of your tool as a whole! Other researchers are still likely encounter errors and problems, but with thorough and easy-to-digest documentation, they are better equipped to troubleshoot these problems! They may also learn more about the features and limitations of the code that will better guide their next steps! This is not only helpful for other researchers but make it more likely that more individuals in the community will use these methods and share them in the community. These types of citations and usage metrics can be valuable to report to funding institutions to describe the impact of the work. Well-documented software help developers better maintain their code in the future because they may forget the mechanics of their software over time. This helps with manuscript revisions, transparency or future research that builds on these methods! References "],["discussion-on-benefits-of-data-sharing.html", "Chapter 6 Discussion on Benefits of Data Sharing 6.1 Discussion", " Chapter 6 Discussion on Benefits of Data Sharing 6.1 Discussion Please see below for a self-directed reflection. Writing grants takes time and can be a lot of work. You might be thinking that adding an additional plan to your proposal is just more paperwork. If applicable, consider some ways that sharing your data and managing it effectively might benefit you in the long term. "],["why-is-there-a-nih-dms-policy.html", "Chapter 7 Why is there a NIH DMS policy? 7.1 Key terms", " Chapter 7 Why is there a NIH DMS policy? First we will discuss the motivations behind the new data management and sharing policy that will go into effect for (most) grant proposals submitted to the NIH after January 25, 2023. Why is the NIH doing this? There are several reasons why sharing data can be beneficial to the scientific community. Supports transparency - Sharing data provides more clarity about how studies are performed. Many scientists also believe in an ethical responsibility to study participants (Bauchner, Golub, and Fontanarosa 2016). Encourages reproducibility and rigor - Having the data accessible, allows others to try to reproduce study findings. This can further enable studies that may replicate or validate the initial findings with different data. Supports multi-modal work - When more data of various types are easily available it makes it easier for scientist to perform studies with multiple types of data (Thessen 2021). More efficient and cost effective - Some data are especially difficult or expensive to produce. Supports Researcher Inclusion - Data generation can be especially difficult for those at institutes with less resources. Publicly available data can therefore be used by these researchers to better enable their participation. Increased impact - Papers that share their data in repositories appear to be cited more based on the study by Colavizza et al. (2020) . Increased collaboration opportunity - Having data available can encourage other researchers to expand the research in a new direction or extend it further and they may reach out to collaborate. Data Citations - Due to the importance of data generation and sharing to the NIH, data will now be seen as research product that demonstrates a contribution to the scientific community. 7.1 Key terms Data Management The work involved with validating, organizing, protecting, maintaining, and processing scientific data to ensure the accessibility, reliability, and quality of the scientific data for use in research. All research data should be actively managed. Data Sharing The act of making scientific data available for use by others (e.g., the larger research community, institutions, the broader public), for example, via an established repository. Some data carry limitations on how data sharing can be done and some meet criteria that make them exempt from data sharing. Metadata Data that provide additional information intended to make scientific data interpretable and reusable. Metadata can include features like dates, independent sample and variable construction and description, methodology, data provenance, data transformations, any intermediate or descriptive observational variables. Data Management and Sharing Plan A plan describing an approach to data management, preservation, and sharing of scientific data and accompanying metadata. References "],["how-will-this-policy-affect-me.html", "Chapter 8 How will this policy affect me? 8.1 Grant Mechanisms 8.2 Grant Renewals 8.3 Impact on Reviews 8.4 Sharing Timeline 8.5 When to Not Share Data", " Chapter 8 How will this policy affect me? Whether you are an investigator applying for a NIH grant, or a researcher or trainee supported by a NIH grant, it is important to know how this policy applies to you and your work. You can refer to the NIH overview of which research will be covered by this new policy. The major requirement of the policy is that all grant proposals (submitted after January 25th, 2023) for mechanisms that require compliance, must include a plan for how they will proactively manage and share their data. For certain grant mechanisms for projects that do not generate data, compliance with the policy is not required. For certain types of data, sharing is not possible, and a justification will be required instead. The following text will discuss several key questions: Is my research exempt from the policy? Does my research generate scientific data? Do grant renewals require compliance with the policy? How will the policy impact the review process? When do I need to share my data? When should I not share data? To determine if your research requires compliance with other policies that may influence how you share your data, take this quiz. In addition to these questions, there are ethical considerations that you may want to think about. See the ethics section of our other course for more information. 8.1 Grant Mechanisms What grant mechanisms require compliance with the DMS policy? The DMS Policy applies to all research that generates scientific data (regardless of the funding level), including: Research Projects Some Career Development Awards (K) Small Business SBIR/STTR Research Centers The DMS Policy does not apply to research and other activities that do not generate scientific data, including: Training (T) Fellowships (F) Construction (C06) Conference Grants (R13) Resource (G) You can look up the NIH Activity Code here to see if a DMS Plan is required for your particular grant type. For example, I am interested in applying to a R03 award. According to the table, a DMS Plan is required for this particular award. 8.1.1 Data-generating Research Does my research generate scientific data? The NIH Data Management and Sharing (DMS) Policy applies to all NIH-supported research generating scientific data. But what is “scientific data”? 8.1.1.1 Scientific data Scientific data are the “recorded factual material of sufficient quality to validate and replicate research findings, regardless of whether the data are used to support scholarly publications”. This can include any of the following if they are applicable to your study: Unpublished results Null results Results used to publish papers 8.1.1.2 Not scientific data You are not expected to share: lab notebooks preliminary analyses case report forms drafts of scientific papers plans for future research peer reviews communications with colleagues physical objects (such as biospecimens) 8.2 Grant Renewals Do grant renewals need to comply with the policy? If you submit a grant renewal application for any of the grants mechanisms that require compliance after January 25th, 2023, then your renewal will need to include a DMS Plan even if the grant was originally funded before January 25th, 2023. 8.3 Impact on Reviews How will this influence the grant review process? For most proposals – those where data sharing is not part of the Notice of Funding Opportunity – the following will happen during the review process: Reviewers for will not have access to your DMS plan. Reviewers will however see your budget which will include some descriptions of how money will be spent to manage and share data. Thus, the DMS plan should not influence your grant score. After a grant receives a fundable score, a Program Officer will review the DMS plan and will work with the PI to address any concerns. Changes based on this process can be made during the Just-in-Time procedures. For proposals where data sharing is specified as part of the Notice of Funding Opportunity the following will happen during the review process: The reviewers will have access to the plan and it may be part of the review criteria. Program staff will also review the DMS plan. 8.4 Sharing Timeline When does the data need to be shared by? Data should be made available no later than publication or end of the award. This means that data underlying findings that are not published in peer-reviewed journals should be made available by the end of the award. 8.4.1 No-cost Extensions Scientific data should be made accessible as soon as possible, and no later than the time of an associated publication or the end of the performance period of the extramural award that generated the data. If a no cost extension is granted for an extramural award, scientific data should be made accessible no later than the time of an associated publication, or the end of the no cost extension, whichever comes first. 8.5 When to Not Share Data Under the NIH DMS Policy, is it possible to not share data? There are legitimate reasons you might not share your data. Data might not be shareable due to ethical, legal, or technical concerns. You will still need to submit a DMS Plan even if you plan to withhold data sharing. You must explain your reasoning in your DMS Plan. Justifiable ethical, legal, and technical factors for limiting sharing of data include: Informed consent will not permit or will limit the scope or extent of sharing and future research use Existing consent (e.g., for previously collected biospecimens) prohibits sharing or limits the scope or extent of sharing and future research use Privacy or safety of research participants would be compromised or place them at greater risk of re-identification or suffering harm, and protective measures such as de-identification and Certificates of Confidentiality would be insufficient. Explicit federal, state, local, or Tribal law, regulation, or policy prohibits disclosure Restrictions are imposed by existing or anticipated agreements with other parties Datasets cannot practically be digitized with reasonable efforts For additional information about potential ethical considerations, including if data is of sufficient quality to be shared. See the ethics section of our other course for more information. What if my data are proprietary? (click for more information) Considerations for Proprietary Data NIH understands that some scientific data generated with NIH funds may be proprietary. Under the Small Business Innovation Research (SBIR) and Small Business Technology Transfer (STTR) Program Policy Directive, effective May 2, 2019, SBIR and STTR awardees may withhold applicable data for 20 years after the award date, as stipulated in the specific SBIR/STTR funding agreement and consistent with achieving program goals. SBIR and STTR awardees are expected to submit a Data Management &amp; Sharing Plan per DMS Policy requirements. Issues related to proprietary data also can arise when co-funding is provided by the private sector (for example, the pharmaceutical or biotechnology industries). NIH recognizes that the extent of data sharing may be limited by restrictions imposed by licensing limitations attached to materials needed to conduct the research. Applicants should discuss projects with proposed collaborators early to avoid agreements that prohibit or unnecessarily restrict data sharing. NIH staff will evaluate the justifications of investigators who believe that they are unable to share data. For questions or concerns about data sharing expectations for proprietary data, please contact the Office of Science Policy. Small businesses may wish to contact the NIH SEED Office. Some additional reasons to limit sharing: NIH respects Tribal sovereignty and supports responsible management/sharing of American Indian / Alaska Native participant data, which can include limiting sharing SBIR/STTR Program Policy Directive permits withholding data for 20 years, as stipulated in agreements and consistent with program goals The following are NOT good reasons to limit the sharing of your data: Data are considered too small Researchers anticipate data will not be widely used Data are not thought to have a suitable repository You don’t have the right personnel to manage data or share data You don’t want to pay for data storage "],["elements-of-the-dms-plan.html", "Chapter 9 Elements of the DMS Plan", " Chapter 9 Elements of the DMS Plan The NIH has provided an outline for what components of the Data Sharing and Management Plan are required. The following are the major elements required to be included a NIH DMS plan: Data type - describe what data (amount and type) will be generated over the course of funding and what data will or will not be shared Tools, software, and code - describe what tools (and versions) you intend to use to manage and analyze the data (note code is not required to be shared) Standards - describe any standards that you might need to use for your data and metadata to make them usable by others or be contributed to a repository Data preservation, access, timelines - describe where the data will be made available and when Access, distribution, reuse considerations - describe how you have carefully considered any reasons that might limit sharing Oversight - describe who will manage compliance of the DMS plan You can find more detailed requirements for each of these elements here. To aid in proactively planning for data management and sharing over the life of your research project, it may be helpful to consider the data types and size you plan to generate, the repositories available, and the corresponding budgetary implications before you begin your research. ## Overall Takeaways Why this change? As of January 23, 2023, the NIH is requiring all grant proposals include a Data Management and Sharing Plan to aid in the transparency and reproducibility of NIH-funded research. Does this apply to me? This will apply to most NIH grants that create data (regardless of funding level), although some grant mechanisms and some data are exempt. It is not required to share all data, however a justification is required. Reasons not to share that are acceptable may be ethical, technical, or legal. Where should I share my data? The NIH has lists of suggested repositories, to help you find appropriate repositories for your data. If there is not an appropriate repository, you can share your data at an institutional (if available) or generalist repository. How do I budget this? You may request funds for data management and sharing as direct costs (including personnel costs). Infrastructure costs should only be included as indirect costs. How do I write my plan? Your plan should be &lt; 2 pages (without hyperlinks) and include sections on data type; tools, software, and code; standards; data preservation, access, and timelines; access, distribution, and reuse considerations; and oversight. You may be able to change your plan in the just-in-time process or during regular reporting intervals. Changes require approval. Will my plan influence my grant score? Although the Data Management and Sharing Plan is a mandatory part of most grant proposals, it will not be shared with reviewers and thus will not influence your score, however the budget will be visible to reviewers. Final Considerations This is a new policy for the NIH and they are expecting to have some growing pains. While they don’t currently have specific expectations, the NIH has stated they will learn from the process as it happens along with the scientists submitting grant proposals under the new policies. In general, it’s best to be mindful of what is actually feasible when it comes to managing and storing your data and try not overstate what you might be able to do. "],["organizing-your-project.html", "Chapter 10 Organizing your project 10.1 Learning Objectives 10.2 Organizational strategies 10.3 Readings about organizational strategies for data science projects: 10.4 Get the exercise project files (or continue with the files you used in the previous chapter) 10.5 Exercise: Organize your project!", " Chapter 10 Organizing your project 10.1 Learning Objectives Keeping your files organized is a skill that has a high long-term payoff. During the initial development of an analysis, you may underestimate how many files and terms you have floating around. But a short time later, you may return to your files and realize your organization was not as clear as you hoped. Tayo (2019) discusses four particular reasons why it is important to organize your project: Organization increases productivity. If a project is well organized, with everything placed in one directory, it makes it easier to avoid wasting time searching for project files such as datasets, codes, output files, and so on. A well-organized project helps you to keep and maintain a record of your ongoing and completed data science projects. Completed data science projects could be used for building future models. If you have to solve a similar problem in the future, you can use the same code with slight modifications. A well-organized project can easily be understood by other data science professionals when shared on platforms such as Github. Organization is yet another aspect of reproducibility that saves you and your colleagues time! 10.2 Organizational strategies There’s a lot of ways to keep your files organized, and there’s not a “one size fits all” organizational solution (Shapiro et al. 2021). In this chapter, we will discuss some generalities but as far as specifics we will point you to others who have written about works for them and advise that you use them as inspiration to figure out a strategy that works for you and your team. The most important aspects of your project organization scheme is that it: Is project-oriented (Bryan 2017). Follows consistent patterns (Shapiro et al. 2021). Is easy for you and others to find the files you need quickly (Shapiro et al. 2021). Minimizes the likelihood for errors (like writing over files accidentally) (Shapiro et al. 2021). Is something maintainable (Shapiro et al. 2021)! 10.2.1 Tips for organizing your project: Getting more specific, here’s some ideas of how to organize your project: Make file names informative to those who don’t have knowledge of the project but avoid using spaces, quotes, or unusual characters in your filenames and folders – these only serve to make reading in files a nightmare in some programs. Number scripts in the order that they are run. Keep like-files together in their own directory: results tables with other results tables, etc. Including most importantly keeping raw data separate from processed data or other results! Put source scripts and functions in their own directory. Things that should never need to be called directly by yourself or anyone else. Put output in its own directories like results and plots. Have a central document (like a README) that describes the basic information about the analysis and how to re-run it. Make it easy on yourself, dates aren’t necessary. The computer keeps track of those. Make a central script that re-runs everything – including the creation of the folders! (more on this in a later chapter) Let’s see what these principles might look like put into practice. 10.2.1.1 Example organizational scheme Here’s an example of what this might look like: project-name/ ├── run_analysis.sh ├── 00-download-data.sh ├── 01-make-heatmap.Rmd ├── README.md ├── plots/ │ └── project-name-heatmap.png ├── results/ │ └── top_gene_results.tsv ├── raw-data/ │ ├── project-name-raw.tsv │ └── project-name-metadata.tsv ├── processed-data/ │ ├── project-name-quantile-normalized.tsv └── util/ ├── plotting-functions.R └── data-wrangling-functions.R What these hypothetical files and folders contain: run_analysis.sh - A central script that runs everything again 00-download-data.sh - The script that needs to be run first and is called by run_analysis.sh 01-make-heatmap.Rmd - The script that needs to be run second and is also called by run_analysis.sh README.md - The document that has the information that will orient someone to this project, we’ll discuss more about how to create a helpful README in an upcoming chapter. plots - A folder of plots and resulting images results - A folder results raw-data - Data files as they first arrive and nothing has been done to them yet. processed-data - Data that has been modified from the raw in some way. util - A folder of utilities that never needs to be called or touched directly unless troubleshooting something 10.3 Readings about organizational strategies for data science projects: But you don’t have to take my organizational strategy, there are lots of ideas out there. You can read through some of these articles to think about what kind of organizational strategy might work for you and your team: Jenny Bryan’s organizational strategies (Bryan and Hester 2021). Danielle Navarro’s organizational strategies Navarro (2021) Jenny Bryan on Project-oriented workflows(Bryan 2017). Data Carpentry mini-course about organizing projects (“Project Organization and Management for Genomics” 2021). Andrew Severin’s strategy for organization (Severin 2021). A BioStars thread where many individuals share their own organizational strategies (“How Do You Manage Your Files &amp; Directories for Your Projects?” 2010). Data Carpentry course chapter about getting organized (“Introduction to the Command Line for Genomics” 2019). 10.4 Get the exercise project files (or continue with the files you used in the previous chapter) Get the Python project example files Click this link to download. Now double click your chapter zip file to unzip. For Windows you may have to follow these instructions. Get the R project example files Click this link to download. Now double click your chapter zip file to unzip. For Windows you may have to follow these instructions. 10.5 Exercise: Organize your project! Using your computer’s GUI (drag, drop, and clicking), organize the files that are part of this project. Organized these files using an organizational scheme similar to what is described above. Create folders like plots, results, and data folder. Note that aggregated_metadata.json and LICENSE.TXT also belong in the data folder. You will want to delete any files that say “OLD”. Keeping multiple versions of your scripts around is a recipe for mistakes and confusion. In the advanced course we will discuss how to use version control to help you track this more elegantly. After your files are organized, you are ready to move on to the next chapter and create a notebook! Any feedback you have regarding this exercise is greatly appreciated; you can fill out this form! References "],["record-keeping-practices.html", "Chapter 11 Record keeping practices", " Chapter 11 Record keeping practices Once you have your project rolling, it is important to keep good records of your work, your collaborators work, and your communication. Keeping good records takes time and discipline but it can save you more time and heartache in the end. Here are some suggestions for how to optimize your record keeping. 11.0.1 Keep organized records of work Record and communicate notes about your data collection and analyses. Be mindful of overwhelming your coworkers, but generally speaking provide extra information where possible. The more people that are aware of details about what samples were in what batch, the more likely important details are not missed or forgotten. For example, if you are sending data to a collaborator, send as much information as possible about how it was generated in the email in which you send it to them, even if you have already discussed the data. This can help ensure that no important details fall through the cracks. The best way we think you can do this in general is to use reports - one of our next suggestions. 11.0.2 Keep organized records of communication Besides recording your work, keep a record of your communications. At a minimum organize your emails for projects into separate folders with easily recognizable titles to save yourself hassle later when something comes into question. However, we highly recommend that in addition for even better record keeping, you can use note-taking system. This could be as simple as a shared Google doc, or you could consider an app like these that are designed for note-taking. With many of these you can also share your notes with research teammates and you can include report documents directly in your notes. Which brings us to our next point about using reports! 11.0.3 Use reports Instead of sending informal short emails (which are useful at some points in a workflow), we suggest intermittently sending lab reports with as much information about what was done and why. For informatics related work in R or Python (or other supported languages) we highly suggest using a method like R markdown or Jupyter notebooks to track what informatics steps you have performed and why. Beginning these reports with a short description of what raw data you used and when you received it can be critical for ensuring that you are using the correct data! We will describe more about how to use such reports in the final chapter of this course. It is also important that the experimental biologists make similar reports defining what reagents they used, when they performed the study, what samples were used, who performed the experiment, and any notes about unusual events, such as the electricity went out during the experiment, left the samples overnight but usually leave two hours, mouse #blank unexpectedly died so we lost this sample thus it is not included, or the dye seemed unusually faint in this gel. In summary, we recommend the following record keeping tips: "],["creating-clear-documentation.html", "Chapter 12 Creating clear documentation 12.1 Characteristics of clear documentation 12.2 Types of documentation you should have 12.3 How to keep your documentation up to date 12.4 Exercise 1: Add a reminder for documentation updates to your task manager 12.5 Exercise 2: Implement a URL checker", " Chapter 12 Creating clear documentation Our goal for documentation is to be as comprehensive, navigable, and as always, as clear as possible. 12.1 Characteristics of clear documentation 12.1.1 Is easy to find No matter how well your documentation is crafted, it is of no use if no one can find them. Having documents that are standard and at the top of your directory is key. READMEs for example are standard documents in software that give the TL;DR of the project. In science, READMEs are incredibly valuable. 12.1.2 Is comprehensive All items are covered in the documentation in an organized fashion – every. single. thing. This includes all: Data sources and versions Metadata Software dependencies and their versions Terms Functions Arguments Parameters Defaults The most useful documentation… Not only define the items and files included, but tells how it relates to other items (and they have links where relevant). Make any existing defaults and calculations very clear. It doesn’t assume that just because a term is used, the calculation is obvious. For example, Tumor Mutation Burden is a common statistic to report but it is calculated different ways. Documentation should describe major calculations and not assume standardization. Shows how to re-run the entire analysis, example lines of code go a long way. Tries to avoid the use of jargon, but if it is absolutely necessary to use a jargon-y term it links to information about the meaning of the term. 12.1.3 Data formats are described Perhaps after installation, getting data formatted correctly is one of the other very large hurdles users will need to deal with. Ideally, your software can use a data format that is common. But the more that your tool is particular about an odd data format, the more your documentation needs to be specific about what the odd data format looks like. It’s very helpful to include subsetted, de-identified example files for a positive control/example. 12.2 Types of documentation you should have Documentation strategies are not one size fits all but there are two types of documentation we strongly advise every project has: READMEs and analysis notebooks. We refer you to see the OpenPBTA project as a real life example of well documented open source data analysis. 12.2.1 READMEs! READMEs are also a great way to help your collaborators get quickly acquainted with the project. READMEs stick out in a project and are generally universal signal for new people to the project to start by READing them. GitHub automatically will preview your file called “README.md” when someone comes to the main page of your repository which further encourages people looking at your project to read the information in your README. Information that should be included in a README: General purpose of the project Instructions on how to re-run the project Lists of any software required by the project Input and output file descriptions. Descriptions of any additional tools included in the project? You can take a look at this template README to get your started. 12.2.1.1 More about writing READMEs: How to write a good README file How to write an awesome README 12.2.2 Exercise: Write a README for your project! Download this template README. Fill in the questions inside the { } to create a README for this project. You can reference the “final” versions of the README, but keep in mind it will reference items that we will discuss in the “advanced” portion of this course. See the R README here and the Python README here. Add your README and updated notebook to your GitHub repository. Follow these instructions to add the latest version of your notebook to your GitHub repository. Later, we will practice and discuss how to more fully utilize the features of GitHub but for now, just drag and drop it as the instructions linked describe. 12.2.3 Notebook descriptions The generous use and keeping of notebooks is a useful tool for documentation of the development of an analysis. Data analyses can lead one on a winding trail of decisions and side investigations, but notebooks allow you to narrate your thought process as you travel along these analyses explorations! Your scientific notebook should include descriptions that describe: 12.2.3.1 The purposes of the notebook What scientific question are you trying to answer? Describe the dataset you are using to try to answer this and why does it help answer this question? 12.2.3.2 The rationales behind your decisions Describe why a particular code chunk is doing a particular thing – the more odd the code looks, the greater need for you to describe why you are doing it. Describe any particular filters or cutoffs you are using and how did you decide on those? For data wrangling steps, why are you wrangling the data in such a way – is this because a certain package you are using requires it? 12.2.3.3 Your observations of the results What do you think about the results? The plots and tables you show in the notebook – how do they inform your original questions? 12.3 How to keep your documentation up to date 12.3.1 The goal of documentation maintenance Perhaps you’ve been making improvements or otherwise updating your software tool. That’s excellent and you deserve a big kudos for continuing maintenance on your tool! But your work is not done yet. For each (user-facing) update you make to the tool, you should also make a documentation update. As a user, the only thing worse than having a tool with no documentation at all is having a tool with documentation that is out of date or otherwise incorrect. If documentation updates aren’t prioritized, your tool can easily get several versions ahead leaving the documentation you carefully crafted rather useless and misleading. 12.3.2 Keep your documentation in one, version-controlled place Presumably you have some sort of process for version controlling your tool updates (we assume GitHub but could be other services). Ideally, your documentation should be version controlled similarly and, if appropriate, in the same place. The easier you make it on yourself to update your documentation, the more likely future you will be at updating it successfully! It’s worth spending time thinking about your own development process and how you can make it easier on yourself and your team for longer-term better maintained documentation. 12.3.3 Do not consider a tool fix done before its relevant documentation update is also completed However you track your tasks, also track your documentation issues and always pair a software fix with a documentation fix – or at least check if it affects anything user-facing. To help you remind you of this, you may want to use an issue template (if you use GitHub) and make sure that issue template includes a reminder to update documentation. 12.3.4 Make sure links work A very simple but all too common problem with out of date documentation is broken links! You can catch these broken links by manually clicking on all your links, but sometimes broken links will still slip through the cracks anyway! There are GitHub actions and other automated tools that can check your URLs for you. Take advantage of automation to do this for you so you can save your time an effort for other improvements to your tool and documentation! Here’s some options for automated URL-checking: GitHub action: urlchecker-action. GitHub action: URL checker. 6 Tools to Find Broken Links on Your Website (Studios 2020). 12.3.5 Set aside time to do maintenance In an academic setting it can be hard to find time for things that don’t have urgent deadlines. But long term we know maintenance is best done little by little. In order to best maintain your work long term its best to set aside time on your calendar to actually do the maintenance. Otherwise it may never happen. We encourage funding institutions to recognize that maintenance is the most frugal strategy. Whether it be for software or course or other products, maintenance should be prioritized for the long term benefit of the research community. More funding opportunities should be set aside for maintenance of current products as opposed to always creating new products that will also decay if not maintained. 12.4 Exercise 1: Add a reminder for documentation updates to your task manager If you use GitHub, add an issue template that has a reminder to update documentation. If you use something else for task management, look for some other way to remind yourself (and your fellow developers on the project) to keep documentation up-to-date for each change. 12.5 Exercise 2: Implement a URL checker If you use GitHub for your documentation, add a url checking GitHub action to your repository. We used a url-checker GitHub action for developing this course! You can see ours here for an example. If you use something else for version control, look into URL checkers that you can easily implement into your development process. References "],["collaborating-with-informatics-experts.html", "Chapter 13 Collaborating with informatics experts", " Chapter 13 Collaborating with informatics experts Studies investigating biology research labs over history indicate that collaboration has been on the rise since the 1950s (vermeulen_understanding_2013?) and that the rate continues to increase (sonnenwald_scientific_2007?). Indeed the size of biology research teams appear to have doubled from 1955 to 1990 (vermeulen_understanding_2013?). But why? 13.0.1 The benefits of collaboration Shared cost Research often involves expensive technology, thus it is cost effective to share resources. Shared expertise Now that technology affords answering in some cases more complex or broader research questions, it is often more effective to employ multiple contributors with different knowledge, skills, and perspectives. Researchers have noted that their own concept of their field changed as a result of working with investigators from other disciplines. Thus this can lead to innovation (makinen_patterning_2020?). Shared burden Doing part of the work for a project using the knowledge and skills that you are most comfortable with and seeking help from others who are more knowledgeable on other research aspects can be a more efficient strategy. Shared reliability Including multiple team members who can each evaluate the research can improve the reliability of a project, as mistakes can be found by other members. Shared credibility Collaborations involving experts of multiple areas can improve the perceived credibility of the work by others. 13.0.2 Potential challenges There are always challenges when collaborating with others, but some of these are particularly enhanced in multi-disciplinary teams. Here are some challenges that you may encounter when a collaboration involves informatics experts. Bad collaboration: Communication Differences Extra care needs to be taken to ensure that communication across groups is effective. Typically researchers will not meet as often with a collaborator as they would with an internal team member. Therefore, poor communication in a collaboration can lead to more costly misdirection and thus wasted time and effort. Furthermore, as investigators often have different backgrounds, differences in jargon and language can make communication more challenging. Having internal team members with some familiarity with informatics can be very beneficial for translating discussions with collaborators who are informatics experts. One solution to this is to have trainees work in both labs. This can be especially beneficial for the trainee who will become accustomed to two research styles and will learn a diverse set of skills. This allows the trainee to potentially have their own multi-disciplinary lab in the future (makinen_patterning_2020?). Another important method that can help resolve this issue is to have members provide educational seminars for participating members about the fundamentals of their work. Different research style and goals Beyond differences in language, differences in research style and goals can lead to conflict. “Scholars’ different styles of thought, standards, research traditions, techniques, and languages can be difficult to translate across disciplinary domains” (makinen_patterning_2020?). Making clear research standards and goals, as well as outlining clear specific tasks at the beginning of a project can help to avoid this issue. Furthermore, meeting consistently throughout the duration of a project can also help to make sure that standards are maintained. Additionally, these meetings should include discussions about intellectual property, authorship, leadership, and defining what success looks likes to each of the various members. Defining these details early can avoid major conflict later. Furthermore, it is critical to keep in mind the diversity of career goals of research team members, as junior team members may have a challenging time persuading others of their independence and contributions when they work on largely collaborative projects. It is also necessary to ensure that junior members have time to devote to their own research programs. (sonnenwald_scientific_2007?) Support should be provided for these junior collaborators by more senior collaborators. Different capabilities Research of multi-disciplinary collaborations has revealed that when collaborating members are unclear of how their expertise and work contributes to the project, they are less motivated and fell less valued. Working with members of different backgrounds to determine how their expertise can contribute to the project, as opposed to simply assigning them a task, will not only help with morale, but it can also better define how a collaborator can further contribute to a project in ways that you may not already expect (makinen_patterning_2020?). Reduced sense of responsibility Another concern of collaboration is that team members may feel less responsibility or commitment to a project than for a project within their own lab. Defining tasks and expected due dates can help reduce this issue. Discussions to establish due dates should always include team members with expertise in each area of science, as tasks may not take the amount of time that another researcher would expect. It is a common misconception that informatics tasks take less time than the tasks actually take in reality. Research is dynamic Research always has an element of trial and error. Protocols may change and new scientific questions may emerge. Frequent meetings with all group members to understand the dynamics of the project are critical. Furthermore, flexibility and understanding is required. It should be expected that aspects about the project will change. Different levels of resources Particularly when collaborating with community members, community colleges, and institutions that are “Equity-oriented” and serving populations that have historically been marginalized or “minoritized” (blake_case_2017?), it is important to keep in mind that large differences in resources may exist between collaborating members. Sharing and discussing budget information early and often can help research members to understand what expectations are reasonable and how collaboration partners may best assist one another. It is also important to recognize that: “There is a common misconception that the lack of physical experimentation and laboratory supplies makes analysis work automated, quick, and inexpensive” (carpenter_cultivating_2021?). However: “In reality, even for well-established data types, analysis can often take as much or more time and effort to perform as generating the data at the bench. Moreover, it typically also requires pipeline optimization, software development and maintenance, and user interfaces so that methods remain usable beyond the scope of a single publication or project” (carpenter_cultivating_2021?). Don’t forget to provide some budget for your informatics collaborators, as their time ultimately does cost money and there may be computational costs that you may not be aware of. "],["guidelines-for-good-metadata.html", "Chapter 14 Guidelines for Good Metadata 14.1 What are metadata? 14.2 How to create metadata?", " Chapter 14 Guidelines for Good Metadata 14.1 What are metadata? Metadata are critically important descriptive information about your data. Without metadata, the data themselves are useless or at best vastly limited. Metadata describe how your data came to be, what organism or patient the data are from and include any and every relevant piece of information about the samples in your data set. Metadata includes but isn’t limited to, the following example categories: At this time it’s important to note that if you work with human data or samples, your metadata will likely contain personal identifiable information (PII) and protected health information (PHI). It’s critical that you protect this information! For more details on this, we encourage you to see our course about data management. 14.2 How to create metadata? Where do these metadata come from? The notes and experimental design from anyone who played a part in collecting or processing the data and its original samples. If this includes you (meaning you have collected data and need to create metadata) let’s discuss how metadata can be made in the most useful and reproducible manner. 14.2.1 The goals in creating your metadata: 14.2.1.1 Goal A: Make it crystal clear and easily readable by both humans and computers! Some examples of how to make your data crystal clear: - Look out for typos and spelling errors! - Don’t use acronyms unless you need to and then if you do need to make sure to explain what the acronym means. - Don’t add extraneous information – perhaps items that are relevant to your lab internally but not meaningful to people outside of your lab. Either explain the significance of such information or leave it out. Make your data tidy. &gt; Tidy data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In tidy data: &gt; - Every column is a variable. &gt; - Every row is an observation. &gt; - Every cell is a single value. 14.2.1.2 Goal B: Avoid introducing errors into your metadata in the future! Toward these two goals, this excellent article by Broman &amp; Woo discusses metadata design rules. We will very briefly cover the major points here but highly suggest you read the original article. Be Consistent - Whatever labels and systems you choose, use it universally. This not only means in your metadata spreadsheet but also anywhere you are discussing your metadata variables. Choose good names for things - avoid spaces, special characters, or within the lab jargon. Write Dates as YYYY-MM-DD - this is a global standard and less likely to be messed up by Microsoft Excel. No Empty Cells - If a particular field is not applicable to a sample, you can put NA but empty cells can lead to formatting errors or just general confusion. Put Just One Thing in a Cell - resist the urge to combine variables into one, you have no limit on the number of metadata variables you can make! Make it a Rectangle - This is the easiest way to read data, for a computer and a human. Have your samples be the rows and variables be columns. Create a Data Dictionary - Have somewhere that you describe what your metadata mean in detailed paragraphs. No Calculations in the Raw Data Files - To avoid mishaps, you should always keep a clean, original, raw version of your metadata that you do not add extra calculations or notes to. Do Not Use Font Color or Highlighting as Data - This only adds to confusion to others if they don’t understand your color coding scheme. Instead create a new variable for anything you might be tempted to color code. Make Backups - Metadata are critical, you never want to lose them because of spilled coffee on a computer. Keep the original backed up in a multiple places. We recommend keeping writing your metadata in something like GoogleSheets because it is both free and also saved online so that it is safe from computer crashes. Use Data Validation to Avoid Errors - set data types to have googlesheets or excel check that the data in the columns is the type of data it expects for a given variable. Note that it is very dangerous to open gene data with Excel. According to (Ziemann2016?), approximately one-fifth of papers with Excel gene lists have errors. This happens because Excel wants to interpret everything as a date. We strongly caution against opening (and saving afterward) gene data in Excel. 14.2.2 To recap: If you are not the person who has the information needed to create metadata, or you believe that another individual already has this information, make sure you get ahold of the metadata that correspond to your data. It will be critical for you to have to do any sort of meaningful analysis! The NIH Data Sharing policy is meant to encourage individuals to create specific and comprehensive plans for sharing their data. Meanwhile this course aims to help individuals create those plans as well as learn other associated skills to aid their research and its benefit to overall scientific efforts of their field. "],["computing-infrastructure.html", "Chapter 15 Computing Infrastructure 15.1 Computing Resources 15.2 Shared Computing Etiquette", " Chapter 15 Computing Infrastructure 15.1 Computing Resources In this chapter we will describe the basics about data size and computing capacity. We will discuss the computing and storage requirements for many types of cancer related data, as well as options to perform informatics work that might require more intensive computing capacity than your personal computer. 15.1.1 Data Sizes Recall that the smallest unit of data is a bit which is either a zero or a one. A group of 8 bits is called a byte, and most computers and phones, and software programs are constructed or designed in a way to accommodate groups of bytes at a time. For example a 32-bit machine can work with 4 bytes at a time and a 64-bit can work with 8 bytes at a time. But how big is a file that is 2 GB? When we sequence a genome, how large is that in terms of binary data? Can our local computer work with the size of data that we would like to work with? First let’s take a look at how the size of binary data is typically described and what this actually means in terms of bits and bytes: Now that we know how to describe binary data sizes, let’s next think about how much computing capacity typical computers have today. 15.1.2 File Sizes Now let’s think about the files that we might need for our research, how big are files typically for genomic, imaging, and clinical research? 15.1.2.1 Genomic data file sizes Genomic data files can be quite large and can require quite a bit of storage and processing power. Here is an image of sizes of some common file types: 15.1.2.2 Imaging Data File Sizes Imaging data, although often smaller than genomic data, can start to add up quickly with more images and samples. Here is an table of average file sizes for various medical imaging modalities from Liu et al. (2017): [source] Note that depending on the study requirements, several images may be needed for each sample. Thus data storage needs can add up quickly. 15.1.2.3 Clinical Data File Sizes Really large clinical datasets can also produce sizable file sizes. For example the Healthcare Cost and Utilization Project (HCUP) National (Nationwide) Inpatient Sample (NIS) contains data on more than seven million hospital stays in the United States with regional information. According to the NIS website it “enables analyses of rare conditions, uncommon treatments, and special populations” (“NIS Database Documentation” n.d.). Looking at the file sizes for the NIS data for different states across years, you can see that there are files for some states, such as California as large as 24,000 MB or 2.4 GB (“NIS Database Documentation” n.d.). You can see how this could add up across years and states quite quickly. 15.1.3 Computing Options 15.1.3.1 Personal computers These are computers that your lab might own, such as a laptop, a desktop, used by one individual or maybe just a few individuals in your lab. If you are not performing intensive computational tasks, it is possible that you will only need personal computers for your lab. However, you may find that this changes, and you might require connecting your personal computers to shared computers for more computational power and or storage. 15.1.3.2 Shared Computing Resources What if you decide that you do need more computational power than your personal computer? You may encounter times where certain informatics tasks take way too long or are not even possible. Evaluating the potential file sizes of the data that you might be working with is a good place to start. However, keep in mind that sometimes certain computations may require more memory than you expect. This is particularly true when working with genomic or image files which are often compressed. So what can you do when you face this issue? One great option, which can be quite affordable is using a server. In terms of hardware, the term server means a computer (often a computer that has much more storage and computing capacity than a typical computer) or groups of computers that can be accessed by other computers using a local network or the internet to perform computations or store data (server_def?). They are often shared by people, and allow users to perform more intensive computational tasks or store large amounts of data. Read here to learn more (server_2021?). Using a group of computers is often a much more cost effective option than having one expensive supercomputer (a computer that individually has the computational power of many personal computers) to act as a server (supercomputer_2022?). It turns out that buying several less powerful computers is cheaper. In some cases however, an institute or company might even have a sever with multiple supercomputers! As an example use of a server, your lab members could connect to a server from their own computers to allow each of them more computational power. Typically computers that act as servers are set up a bit differently than our personal computers, as they do not need the same functionality and are designed to optimize data storage and computational power. For instance they often don’t have capabilities to support a graphical user interface (meaning the visual display output that you see on your personal computer) (GUI?). Instead they are typically only accessed by using a command-line interface, meaning that users write code instead of using buttons like they might for a program like Microsoft Word that uses a graphical user interface (command-line_2022?). In order to support this they have memory, processors or CPUs, and storage like your laptop. Here is what a server might look like: In this case we have a group of computers making up this server. Here we see the nodes (the individual computers that make up the server) stacked in columns. Among shared computing resources/servers there are three major options: Clusters - institutional or national resources Grids - institutional or national resources Cloud - commercial or national resources 15.1.3.3 Computer Cluster In a computing cluster several of the same type of computer (often in close proximity and connected by a local area network with actual cables or an intranet rather than the internet) work together to perform pieces of the same single task simultaneously (computer_cluster_2022?). The idea of performing multiple computations simultaneously is called parallel computing (parallel_2021?). There are different designs or architectures for clusters. One common one is the Beowulf cluster in which a master computer (called front node or server node) breaks a task up into small pieces that the other computers (called client nodes or simply nodes) perform (beowulf_2022?). For example, if a large file needs to be converted to a different format, pieces of the file will be converted simultaneously by the different nodes. Thus each node is performing the same task just with different pieces of the file. The user has to write code in a special way to specify that they want parallel processing to be used and how. See here for an introduction about how this is done (Zach_Caceres_GNU_Parallel_2019?). It is important to realize that the CPUs in each of the node computers connected within a cluster are all performing a similar task simultaneously. See here for more information (de_doncker?). 15.1.3.4 Computer Grid In a computing grid are often different types of computers in different locations work towards an overall common goal by performing different tasks (grid?). Again, just like computer clusters, there are many types of architectures that can be rather simple to very complex. For example you can think of different universities collaborating to perform different computations for the same project. One university might perform computations using gene expression data about a particular population, while another performs computations using data from another population. Importantly each of these universities might use clusters to perform their specific task. Both grids and clusters use a special type of software called middleware to coordinate the various computers involved. Users need to write their scripts in a way that can be performed by multiple computers simultaneously. Users also need to be conscious of how to schedule their tasks and to follow the rules and etiquette of the specific cluster or grid that they are sharing (more on that soon!). See here and herefor more information about the difference between clusters and grids (lithmee_difference_2018?; grid_cluster_difference_2019?). 15.1.3.5 “Cloud” computing More recently, the “Cloud” has become a common computing option. The term “cloud” has become a widely used buzzword (cha_cloud_2015?) that actually has a few slightly different definitions that have changed overtime, making it a bit tricky to keep track of. However, the “cloud” is typically meant to describe large computing resources that involve the connection of multiple servers in multiple locations to one another (cloud_2022?) using the internet. See here for a deeper description of what the term cloud means today and how it compares to other more traditional shared computing options (cloud_deeper?). Many of us use cloud storage regularly for Google Docs and backing up photos using iPhoto and Google. Cloud computing for research works in a similar way to these systems, in that you can perform computations or store data using an available server that is part of a larger network of servers. This allows for even more computational dependability beyond a more simple cluster or grid. Even if one or multiple servers is down, you can often still use the other servers for the computations that you might need. Furthermore, this also allows for more opportunity to scale your work to a larger extent, as there is generally more computing capacity possible with most cloud resources (cloudvstrad?). Companies like Amazon, Google, Microsoft Azure, and others provide cloud computing resources. Somewhere these companies have clusters of computers that paying customers use through the internet. In addition to these commercial options, there are newer national government funded resource options like Jetstream (described in the next section). We will compare computing options in another chapter coming up. 15.1.3.6 Accessing Shared Computer Resources It’s important to remember that all of the shared computing options that we previously described involve a data center where are large number of computers are physically housed. You may have access to a HPC (which stands for High Performance Computing) cluster at your institute. This can be a great cost-effective and typically secure option. If your university or institution has a HPC cluster, this means that they have a group of computers acting like a server that people can use to store data or assist with intensive computations. Often institutions can support the cost of many computers within an HPC cluster. This means that multiple computers will simultaneously perform different parts of the computing required for a given task, thus significantly speeding up the process compared to you trying to perform the task on just your computer! If your institute doesn’t have a shared computing resource like the HPCs we just described, you could also consider a national resource option like Xsede. Xsede is led by the University of Illinois National Center for Supercomputing Applications (NCSA) and includes 18 other partnering institutions (which are mostly other universities). Through this partnership, they currently support 16 supercomputers. Universities and non-profit researchers in the United States can request access to their computational and data storage resources. See here for descriptions of the available resources. [source] Stampede2, generously funded by the National Science Foundation (NSF) through award ACI-1134872, is one of the Texas Advanced Computing Center (TACC), University of Texas at Austin’s flagship supercomputers. Importantly when you use shared computers like national resources like Stampede2 available through Xsede, as well as institutional HPCs, you will share these resources with many other people and so you need to learn the proper etiquette for using and sharing these resources. 15.2 Shared Computing Etiquette In this chapter we will discuss the proper etiquette for using more traditional shared computing resources, such as an institutional high performance computing cluster server. This will help you to understand what would be required for you to use such resources. Different resources will have slightly different use rules, however, many resources will share common usage requirements. The following is written based on personal experience, the (doi_rules?) and the (JHPCE?). We will use the Johns Hopkins Joint High Performance Computing Exchange (JHPCE) cluster resource as an example to motivate the need for usage rules and proper sharing etiquette for such resources. First let’s learn a bit about this JHPCE. For this particular resource there are about 400 active users.It is optimized for genomic and biomedical research and has 4,000 cores! That’s right, as you can imagine, this is much more powerful than the individual laptops and desktops that researchers at the university have for personal use, which would typically currently only have around 8 cores. There is also 28TB of RAM and 14 PB of storage! Now that you know more about digital sizes, you can appreciate that this server can allow for much faster processing and really large amounts of storage, as again a researchers’ computer might have something like 16 GB of RAM and 1TB of storage. There are 68 nodes that make up the JHPCE currently. As, with most clusters some of the nodes are dedicated to managing users logging in to the cluster and some of the nodes are dedicated to data transferring. Each node has 2-4 CPUs that provide 24-128 cores! As you can see these processors or chips have a lot more cores per each CPU than a typical personal computer. Individual users connect and perform jobs (aka computational tasks) on the cluster using a formal common pool resource (CPR) hierarchy system. What does this mean? This means that it is a shared resource, where if one user overused the resource it would be to the detriment of others and to overcome this there are usage rules and regulations that are enforced by managers of the resource (common-pool_2022?). This is important because if a single or a few users used up all the computing resources one day, then the other nearly 400 users would have to delay their work that day, which would not be fair. 15.2.1 General Guidelines for shared computing resources Each cluster or other shared computing resource will have different rules and requirements, but here are a few general rules to keep in make sure that you don’t accidentally abuse the privilege of sharing an amazing resource like this. Don’t be too worried, most shared resources will give you guidance about their specific rules and will often also have settings that don’t allow users to make major blunders. 15.2.1.1 Security guidelines One major aspect to consider is keeping the computers in the cluster safe from harm. You wouldn’t want to lose your precious data stored on the cluster and neither would your colleagues! Use a good secure password that is not easy for someone else to guess. Some people suggest using sentences that are easy for you to remember, you could consider a line of lyrics from song or poem that you like, or maybe a movie. Modify part of it to include symbols and numbers (passwords?). Don’t share your password and keep it safe! If you have a Mac, you could consider storing it in your Keychain, alternatively if you have a different type of computer or don’t like the Mac Keychain, consider Dashlane or other password manger services. Luckily both of these options do not come at any extra cost and can be helpful for storing all the passwords we use regularly safely. These are especially good options if your password is difficult for you to remember. Make sure that you abide by any rules regarding storing passwords that might be required by the resource you intend to use. Don’t access a server on a computer that is not authorized to do so. Some servers will require that your computer be authorized for access for added security. It’s a good idea to follow these rules. If you can, perhaps authorize a laptop in case you might need to gain access when you need to be out of town. However if you do so, make sure you also only access such servers with a secure WiFi network. One way to ensure this is is to avoid using public WiFi networks. If you must use a public WiFi network, consider using a virtual private network (VPN) for added security. Here is an article about different VPN options (gilbertson_4_2021?). Do not alter security settings without authorization. Loosening security settings could pose a risk to the data stored on the server. On the other hand, making more strict security settings could cause other users to not be able to perform their work. Contact the managers of the resource if you think changes need to be made. Immediately report any data security concerns. To protect the integrity of your data and your colleagues, be sure to report anything strange about the shared computing resource to those who manage it so that they can address it right away. Also report to them if you have any security breaches on the computer(s) that you use to access the shared computing resource. 15.2.1.2 Overall use guidelines Now that we know how to keep the resource safe, let’s next talk about general usage. Don’t install software unless you have permission. It is possible that the software you want to use might already be installed somewhere on the shared computing resource that you are unaware about. In addition, if you install a different version of a software program, it is possible that this version (especially if it is newer) will get automatically called by other people’s scripts. This could actually break their scripts or modify their results. They may have a reason to use an older version of that software, do not assume that they necessarily want the updated version. Instead, let the managers of the resource know. They can inform other users and make sure that everyone’s work will not be disrupted. Don’t use the server for storage or computation that you are not authorized for. This is often a rule for shared computing resources, simply because such shared resources are intended for a specific reason and likely funded for that reason. Such resources are costly, and therefore the computational power should be used only for what it is intended for, otherwise people may view the use of the resources for other purposes as essentially theft. Don’t alter configurations without authorization. This could result unintended and unexpected consequences for other users. 15.2.1.3 Daily use guidelines Now let’s discuss how you should use such resources on a daily basis. When you submit jobs, make sure you follow the following guidelines. Again consider the fact that there may be more or different requirements for the specific resource that you might be using. Don’t use the login or transfer nodes for your computations. This will cause issues for other users in terms of logging in and transferring their data. This could cause them to be unable to do their work. Think about memory allocation and efficiency. Consider how much RAM and storage is available for people on the shared computing resource. Try not to overload the resource with a really intensive job or jobs that will use most of the resources and either slow down the efficiency of the work for others or not allow them to perform their work at all. This involves: Not using too many nodes if you don’t need to Not using too much RAM on a given node or overall if you don’t need to Not submitting too many jobs at once Communicating with others to give them advanced warning if you are going to submit large or intensive jobs If you have a really large job that you need to perform, talk with the managers of the resource so that you can work out a time when perhaps fewer users would be inconvenienced. Consult the guidelines for your particular resource about how one let’s people know about large jobs before you email the administrators of the resource directly. Often their are communications systems in place for users to let each other know about large jobs. 15.2.1.4 Communication Guidelines Speaking of communication, let’s dive into that deeper for a bit. Use the proper order for communication. Often shared resources have rules about how they want people to communicate. For example for some resources it is suggested that you first ask your friends and colleagues if you are confused about something, then consult any available forums, if that does not work then directly email the administrators/managers of the resource. Keep in mind that these people are very busy and get lots of communications. Use the ticket system If a resource has a ticket system for users to get support, use it instead of communicating by email. If such a system is in place, then the administrators running it are used to getting requests this way. If you email directly, you may not receive feedback in a timely manner or the email might get lost. 15.2.1.5 Specific Rules Ultimately it is very important to learn about the rules, practices, and etiquette for the resource that you are using and to follow them. Otherwise, you could lose access. Often other users are a great resource! 15.2.2 Interacting with shared resources Often you will need to use the command line to interact with a server from your personal computer. To do so on a Mac or a Linux computer you can typically do so using the terminal program that is already on your computer. For PC or Windows computer users, you can use programs like MobaXterm. If you wish to run a program with a graphical interface, then you might need to have a program to help you do so. On Macs, you can download XQuartz. If you use MobaXterm on your PC or Windows computer, then you will already be set. Linux computers also typically should already have what you need. If you are new to Unix commands check out this cheat sheet below. 15.2.3 Running Jobs Typically a program is used to schedule jobs. Remember that jobs are the individual computational tasks that you ask the server to run. For example, this could be something as simple as moving large files from one directory to another or as complex as running a complicated script on a file. Such job scheduling programs assign jobs to available node resources as they become available and if they have the required resources to meet the job. These programs have their own commands for running jobs, checking resources, and checking jobs. Remember to use the management system to run your jobs using the compute nodes not the login nodes (nodes for users to log in). There are often nodes set up for transferring files as well. In the case of the JHPCE, a program called Sun Grid Engine (SGE) is used, but there are others job management programs. See here for more information on how people use SGE for the JHPCE shared resource. 15.2.3.1 Specifying memory (RAM) needs Often there is a default file size limit for jobs. For example the JHPCE has a 10GB file size limit for jobs. You may need to specify when you have a job using a file that exceeds the file size limit and set the file size for that job. As you may recall if you are using whole genome files you are likely to exceed the default file limit size. Often you are also given a default amount of RAM for your job as well. Again, you can typically run a job with more RAM if you specify. Similar to the file size limit, you will likely need to set the RAM that you will need for your job if it is above the default limit. Often this involves setting a lower and upper limit to the RAM that your job can use. If your job exceeds that amount of RAM it will be stopped. Typically people call stopping a job “killing” it. The lower and upper limit can be the same number. How do you know how much RAM to assign to your job? Well if you are performing a job with files that are two times the size of the file size default limit, then it might make sense to double the RAM you would typically use. It’s also a good idea to test on one file first if you are going to perform the same job on multiple files. You can then assess how much RAM the job used. First try to perform the job with lower limits and progressively increase until you see that the job was successful and not killed for exceeding the limit. Keep in mind however how much RAM there is on each node. Remember, it is important to not ask for all the RAM on a single node or core on that node, as this will result in you hogging that node and other users will not be able to use RAM on that node or core on that node. Remember that you will likely have the option to use multiple cores, this can also help you to use less RAM across each core. For example, a job that needs 120GB of RAM could use 10 cores with 12 GB of RAM each. Often there will be a limit for the number of jobs, the amount of RAM, and the number of cores that a single user can use beyond the default limits. This is to ensure that a user doesn’t use too many resources causing others to not be able to perform their jobs. Check to see what these limits are and then figure out what the appropriate way is to contact to request for more. Again communication standards and workflows may vary based on the resource. 15.2.3.2 Checking status It’s also a good idea to check the status of your jobs to see if they worked or got killed. You can check for the expected file outputs or there are commands for the server management software that can help you check currently running jobs. 15.2.4 Storage Often you will be given a home directory which will likely be backed up, however, other storage directories often will not be. Be careful about where you store your data, as some directories might be for temporary use and get wiped to keep space available for others. 15.2.5 Research Platforms In this chapter we will provide examples of computing platforms that are designed to help researchers and that you might find useful for your work. Please note that we aim to provide a general overview of options and thus this is not a complete list. Let us know if there is a platform or system that you think we should include! We highly suggest you also read the next chapter, which will point out important considerations to think about when deciding to work on a shared computing resource platform like those discussed in this chapter. The major advantage of these platforms is that users can analyze data where it lives, as many platforms host public data. However, some also allow you to upload your own data. There is less need for data transfers back and forth to your personal computer, as you can analyze your data, store your data and share it in one place, saving time. Users can sometimes also share how they did their analysis as well, improving reproducibility practices. Additionally, another advantage is that some of these platforms also provide educational material on how to work with data. Many offer a graphical user interface also simply called just graphical interface or GUI, allows for users to choose functions to perform by interacting with visual representations, which can be useful for individuals how are less comfortable writing code. They have a “user-centered” design that creates a visual environment where users can for example click on tabs, boxes, or icons for to perform functions. This also often allows users to more directly see plots and other types of visualizations. Some platforms also offer a command line interface (also known as a character interface) which allows for software functions to be performed by specifying through commands written in text. This typically offers more control than a GUI, however command line interfaces are often less user friendly as they require that the user know the correct commands to use. 15.2.5.1 National Cancer Institute Cloud Resources Funded by the National Cancer Institute (NCI), the cancer research data commons provides data access and computing infrastructure for researchers through three different platforms, the Cancer Genomics Cloud (CGC) which uses Google Cloud resources, the Institute for Systems Biology Cancer Gateway in the Cloud (ISB-CGC) which also uses Google Cloud resources, and FireCloud from the Broad Institute, which uses Amazon Cloud resources. 15.2.5.2 Cancer Genomics Cloud The Cancer Genomics Cloud (CGC) is a computing platform that researchers can used to analyze, store, and share their own data, as well as work with large public and controlled cancer data sets, including genomic and imaging data. CGC offers tutorials and guides to help research get started, as well as $300 of free credits to use the platform and test it out. Users can also access many tools and workflows to help them perform there analyses. CGC also offers regular webinars. The platform is based on a partnership with Seven Bridges, a biomedical analytics company, and can be accessed simply by using a web browser. Users can can use a point and click system also called a graphical user interface (GUI) or can access resources using the command line. See this link to learn more. 15.2.5.3 Institute for Systems Biology (ISB) Cancer Gateway in the Cloud The ISB-CRC platform allows users to browse and data from the Genomic Data Commons and other sources, including sequencing and imaging data both public and controlled. They provide access pipeline tools, as well as to pipelines, workflows, and Notebooks written by others in R and Python to help users perform analyses. ISB also offers $300 in free credits to try out the platform. See here for a user guide. 15.2.5.4 Broad Institute FireCloud FireCloud provides users with computing resources and access to workspaces using Broad’s tools and pipelines. Users can run large scale analyses and work with collaborators. FireCloud offers access to The Cancer Genome Atlas (TCGA) controlled-access data. Other platforms like Galaxy and Terra described next, share resources with FireCloud. 15.2.5.5 Galaxy This section was written by Jeremy Goecks: Galaxy is a web-based computational workbench that connects analysis tools, biomedical datasets, computing resources, a graphical user interface, and a programmatic API. Galaxy (https://galaxyproject.org/) enables accessible, reproducible, and collaborative biomedical data science by anyone regardless of their informatics expertise. There are more than 8,000 analysis tools and 200 visualizations integrated into Galaxy that can be used to process a wide variety of biomedical datasets. This includes tools for analyzing genomic, transcriptomic (RNA-seq), proteomic, metabolomic, microbiome, and imaging datasets, tool suites for single-cell omics and machine learning, and thousands of more tools. Galaxy’s graphical user interface can be used with only a web browser, and there is a programmatic API for performing scripted and automated analyses with Galaxy. Galaxy is used daily by thousands of scientists across the world. A vibrant Galaxy community has deployed hundreds of Galaxy servers across the world, including more than 150 public and three large national/international servers in the United States, Europe, and Australia (https://usegalaxy.org, https://usegalaxy.eu, https://usegalaxy.org.au). The three national/international servers have more than 250,000 registered users who execute &gt;500,000 analysis jobs each month. Galaxy has been cited more than 10,000 times with &gt;20% from papers related to cancer. The Galaxy Tool Shed (https://usegalaxy.org/toolshed) provides a central location where developers can upload tools and visualizations and users can search and install tools and visualizations into any Galaxy server. Galaxy has a large presence in the cancer research community. Galaxy serves as an integration and/or analysis platform for 7 projects in the NCI ITCR program. There is also increasing use of Galaxy in key NIH initiatives such as the NCI Cancer Moonshot Human Tumor Atlas Network (HTAN) and the NHGRI Data Commons, called the AnVIL (https://anvilproject.org/). Galaxy’s user interface, accessible via a web browser, provides access to all Galaxy functionality. The main Galaxy interface has three panels: available tools (left), running analyses and viewing data (middle), and a full history of tools run and datasets generated (right). Datasets for analysis in Galaxy can be uploaded from a laptop or desktop computer or obtained from public data repositories connected to Galaxy. With Galaxy, complex workflows composed of tens or even hundreds of analysis tools can be created and run. In Galaxy’s workflow interface, tools can be added and connected via a simple drag-and-drop approach. Galaxy users can share all their work—analysis histories, workflows, and visualizations—via simple URLs that are available to specific colleagues or a link that anyone can access. Galaxy’s user interface is highly scalable. Tens, hundreds, or even thousands of datasets can be grouped into collections and run in parallel using individual tools or multi-tool workflows. In summary, Galaxy is a popular computational workbench with tools and features for a wide variety of data analyses, and it has broad usage in cancer data analysis. See here for the list of applications supported by Galaxy and here for more information on how to use Galaxy resources. 15.2.5.6 Terra Terra is a biomedical research computing platform that is based on the Google Cloud platform, that also allows users easier ways to manage the billing of their projects. It provides users with access to data, workflows, interactive analyses using Jupyter Notebooks, RStudio, and Galaxy, data access and tools from FireCloud from the Broad Institute, as well as workspaces to organize projects and collaborate with others. Terra also has many measures to help ensure that data is secure and they offer clinical features for ensuring that health data is protected. Note that users who do upload protected health information must select to use extra clinical features and enter a formal agree with Terra/FireCloud about their data. See here for more information. Importantly users can get access to use Genotype -Tissue Expression (GTEx), Therapeutically Applicable Research to Generate Effective Treatments (TARGET) and The Cancer Genome Atlas (TCGA) data using the platform. See here for information on how. Users can pay for data storage and computing costs for Google Cloud through Terra. Users can browse data for free. Check out this video for more information: 15.2.6 AnVIL If you could use some guidance on how to perform analyses using Galaxy and Terra, especially for genomic research, check out AnVIL, the National Human Genome Research Institute (NHGRI) Analysis Visualization and Informatics Lab-space. It also provides access to many important genomic and related datasets from the NHGRI. According to their website: By providing a unified environment for data management and compute, AnVIL eliminates the need for data movement, allows for active threat detection and monitoring, and provides elastic, shared computing resources that can be acquired by researchers as needed. It relies on Terra for the cloud based compute environment, Dockstore for standardized tools and workflows, Gen3 for data management for querying and organizing data, Galaxy tools and environment for analyses with less code requirements, and Bioconductor tools for R programming users. Bioconductor is a project with the mission to catalog, support, and disseminate bioinformatics open-source R packages. Packages have to go through a review process before being included. 15.2.7 CyVerse CyVerse is a similar computing platform that also offers computing resources for storing, sharing, and working with data with a graphical interface, as well as an API. Computing was previously offered using the cloud computing platform from CyVerse called Atmosphere, which relied on users using virtual machines. Users will now use a new version of Atmosphere with partnership with Jetstream. This allows users to use containers for easier collaboration and also offers US users more computing power and storage. Originally called iPlant Collaborative, it was started by a funding from the National Science Foundation (NSF) to support life sciences research, particularly to support ecology, biodiversity, sustainability, and agriculture research. It is led by the University of Arizona, the Texas Advanced Computing Center, and Cold Spring Harbor Laboratory. It offers access to an environment for performing analyses with Jupyter (for Python mostly) and RStudio (for R mostly) and a variety of tools for Genomic data analysis. See here for a list of applications that are supported by CyVerse. Note that you can also install tools on both platforms. Both CyVerse and Galaxy offer lots of helpful documentation, to help users get started with informatics analyses. See here to learn more. 15.2.8 SciServer SciServer is accessible through a web browser and allows users to store, upload, download, share, and work with data and common tools on the same platform. It was originally built for the astrophysics community (and called SkyServer) but it has now been adapted to be used by scientists of all fields and is indeed used by many in the genomics field. It allows users to use Python and R in environments like Jupyter notebooks and RStudio, and also supports (Structured Query Language) SQL for data querying and management and is built on the use of Docker. The main idea of SciServer, is based on this premise: “bring the analysis to the data”. It is free to use after users register. However, users can buy extra resources. Users can keep data private or share their data. As compared to Galaxy, this resources may be better for users with a bit more familiarity with informatics but who require more flexibility, particularly for working with collaborators such as physicists or material scientists as there are more tools supported across disciplines. In addition it also gives users access to very large data sets on Petabyte-scale (note that some of these require special permission to use) and supports developers to create their own web interfaces called SciUIs for particular use cases. For (sciserver_2020?) for more information. 15.2.9 Materials Cloud Another resource that might be of interest to Python users, particular those who collaborate with material scientists, is Materials Cloud. It is designed to promote reproducible work, collaboration, and sharing of resources among scientists, particularly for simulations for the materials science field. Users can share data in a citable way, download data, upload data, share workflows, and perform analyzes. This resource uses AiiDAlab as the computing environment for researchers, which is based on AiiDA. According to their website: AiiDAlab builds on AiiDA as the computational workflow engine, and the Jupyter environment (notebooks, widgets, …) for writing and sharing apps See here to learn more about how AiiDAlab supports the sharing of scientific workflows, particularly for those that use Python. To learn more about Materials Cloud, check out (talirz_materials_2020?). 15.2.10 Overture Overture is a relatively new option for perform large-scale genomic data analyses. You can upload, download, manage, analyze and share your data with authentication and authorization methods to add security. Although designed for genomic research, the data management system can be used for other scientific domains. Currently, additional products are still being developed for analysis, visualization, and sharing. However, several collaborations have created new incredible resources using some of the existing and developing products that might be useful for your research. Alternatively, Overture has options to help you create your own platform, see here for more information. It is compatible with Google, Microsoft Azure, and PostgreSQL for storage options. These collaborations using Overture products can be found on the case studies page of the Overture website. For example, the Cancer Genome Collaboratory is one such collaboration. This is A cloud-based resource that allows researchers to perform analyses using International Cancer Genome Consortium (ICGC) cancer genome data, which includes tumor mutation data from the The Cancer Genome Atlas (TCGA) and the Pan-Cancer Analysis of Whole Genomes (PCAWG) mutation data. See here for information about billing, storage capacity, access, and security. In addition, Overture products have also been used to create other data resources, such as the Kids First Data Resource Portal which has childhood cancer and birth defect genomic data for over 76,000 samples, and the National Cancer Institute’s Genomic Data Commons Data portal, which also includes The Cancer Genome Atlas (TCGA) and Therapeutically Applicable Research to Generate Effective Treatments (TARGET). The portal supports some basic analyses as well for clinical data statistics and survival analysis. 15.2.11 Globus Globus provides developers of new platforms to manage, transfer, and share data with special attention to privacy and security. It has been used for several platforms such as the Systems Biology Knowledgebase (KBase), which is focused on integrating data across plants and microbes, Biomedical Research Informatics Network (BIRN), which is a collaborative project to bring biomedical researchers together and share resources and data (helmer_enabling_2011?), and Globus Genomics,which uses the Globus data management infrastructure, Amazon web services, and Galaxy workflows to assist researchers with their genomics research endeavors. See this link for more examples of how others have used Globus. 15.2.12 BaseSpace Sequence Hub BaseSpace is a platform that allows for data analysis of Illumina sequencing data and syncs easily with any Illumina sequencing machines that you might work with. There are many applications available to help you with your genomics research. They offer a 30 day free trial. 15.2.13 ATLAS.ti ATLAS.ti is designed particularly for qualitative analysis. You can use a variety of data types including video, audio, images, surveys, and social media data. A variety of tools, particularly for text data analysis are provided for methods such as sentiment analysis, which is the process of assigning a general tone or feeling to text and named-entity recognition, which is the process of extracting certain characteristics from texts that are what is called a [named entity] or a real-world object - such as a person’s name or address. Such analyses can be helpful for understanding behaviors that might be associated with cancer risk. Although this type of analysis can be performed using R or Python among other coding languages, ATLAS.ti offers a nice graphical user interface to perform these types of analyses.Furthermore ATLAS.ti offers a great deal of flexibility about such analyses using different data types easily. 15.2.14 GenePattern GenePattern is similar to Galaxy in that it provides a web-based interface for genomic analyses. You can upload your own data, use workflows and pipelines form others and more! See here to access their user guide and here for a quick start guide to using GenePattern Notebook which uses Jupyter Notebooks and GenePattern analysis tools to easily create data analysis reports. Users can also publish and share their notebooks with collaborators or the field, as well as access other people’s notebooks that they can adapt for their own uses. See here for a collection of available notebooks. 15.2.15 XNAT XNAT offers computing resources and tools for performing imaging analysis and for storing and sharing imaging data in a HIPAA complaint manner (more on that in the coming). Developed by the Bukner lab previously at the Washington University and now at Harvard, it supports a variety of imaging data as well as other data types like clinical data. Some tools can be used with a graphical interface and others with the command-line. See here for example use cases. There is also a great deal of documentation available about how to use the tools and resources available at https://wiki.xnat.org/documentation. 15.2.16 OHIF The open health imaging foundation (OHIF) is a web-based imaging analysis platform that is widely used, particularly for radiology analysis, but it also supports whole-slide microscopy image analysis. It was developed by Gordon Harris et al. and can be used for a variety of applications from cardiology to veterinary medicine. Check out these example use cases of OHIF. OHIF also provides thorough documentation with images and videos about how to use the image viewer and tools available. For those interested, Gordon Harris and others are also working on a project called Cornerstone, with the goal of providing software for others to display medical images in web browsers. 15.2.17 PRISM The Platform for Imaging in Precision Medicine called PRISM works behind the scenes in the Cancer Imaging Archive (TCIA) to allow users to work with the vast data available in TCIA, in terms of both imaging data and clinical information. According to Fred Prior: It is designed to collect, curate and manage Radiology and Pathology images, clinical information associated with those images, annotations and image derived feature sets. PRISM is designed to run on a Kubernettes cluster, but all of the components are containerized so they can run stand-alone or in an alternate orchestration framework. See this article for more information. References "],["data-storage-and-data-repositories.html", "Chapter 16 Data Storage and Data Repositories 16.1 Data Storage Concerns 16.2 Data Repositories 16.3 Considerations for Human Data 16.4 No Existing Repository? 16.5 Data repositories 16.6 Finding a repository", " Chapter 16 Data Storage and Data Repositories To enable a more productive research experience and to ease your future compliance with any data sharing requirements, active data management during the research process can be very helpful. 16.1 Data Storage Concerns How should you store and interact with data while doing research? Your data storage needs will depend greatly on the type of data you will be working with as well as the number of samples you might have. File sizes vary considerably based on the data being stored. A single file can be as small as 1 MB in size (for a PET scan image of the heart) to 60 GB (for an uncompressed fastq of a whole genome sequence). Storing files in a compressed format, especially raw files, can help decrease your storage needs and costs. Below is a table of common types of data and sizes for single files. This list is not comprehensive but instead should be taken as a general guide. You should always get a more specific estimate for your particular project before submitting your grant proposal. Type of data Common size for a single file Genomics (WGS, WES) 15-60 GB Genomics (RNA-seq, scRNA-seq) 3-25 GB Imaging (microscopy) 2-8 MB Imaging (human medical) 1 MB – 2.5 GB Flow cytometry 1-50 MB Proteomics 3-5 MB Clinical trials 2-25 MB As you can see some of these data types require large data files. These files may quickly add up to require more storage or computing capacity than your laptop (which typically have 250 GB-1 TB of storage)! If you want to learn more about data file sizes check out the data file size details section of the appendix and this class on Computing for Cancer Informatics from the Informatics Technology for Cancer Research (ITCR) Training Network (ITN) for options on how to manage large data files. 16.2 Data Repositories Where will I share my data? What repositories exist that might work for my data type? Some programs or Funding Opportunity Announcements (FOA) will specify where the data should be shared. If this applies, you should plan to use the repositories mentioned in the FOA. Other programs or FOAs will not specify where the data should be shared, however the NIH provides an interactive table of NIH-supported data repositories to help you identify repositories that might be appropriate for your data. If you don’t find a repository there, additional repositories can also be found at the following links: Open NIH-supported domain-specific repositories Other NIH-supported domain-specific resources Nature data sharing resources Registry of research data repositories Researchers should aim to find repository with the following characteristics according to the NIH: Established: If the repository is established (well-known or has been around for a significant period of time), it is likely to improve the FAIRness (Wilkinson et al. 2016) of the data. Specific: Repositories that are discipline or data-type specific should be prioritized to promote reuse. Unique Persistent Identifiers: Assigns datasets a citable, unique persistent identifier, such as a digital object identifier (DOI) or accession number, to support data discovery, reporting, and research assessment. The identifier points to a persistent landing page that remains accessible even if the dataset is de-accessioned or no longer available. Long-Term Sustainability: Has a plan for long-term management of data, including maintaining integrity, authenticity, and availability of datasets; building on a stable technical infrastructure and funding plans; and having contingency plans to ensure data are available and maintained during and after unforeseen events. Metadata: Ensures datasets are accompanied by metadata to enable discovery, reuse, and citation of datasets, using schema that are appropriate to, and ideally widely used across, the community/communities the repository serves. Domain-specific repositories would generally have more detailed metadata than generalist repositories. Curation and Quality Assurance: Provides, or has a mechanism for others to provide, expert curation and quality assurance to improve the accuracy and integrity of datasets and metadata. Free and Easy Access: Provides broad, equitable, and maximally open access to datasets and their metadata free of charge in a timely manner after submission, consistent with legal and ethical limits required to maintain privacy and confidentiality, Tribal sovereignty, and protection of other sensitive data. Broad and Measured Reuse: Makes datasets and their metadata available with broadest possible terms of reuse; and provides the ability to measure attribution, citation, and reuse of data (i.e., through assignment of adequate metadata and unique identifiers). Clear Use Guidance: Provides accompanying documentation describing terms of dataset access and use (e.g., particular licenses, need for approval by a data use committee). Security and Integrity: Has documented measures in place to meet generally accepted criteria for preventing unauthorized access to, modification of, or release of data, with levels of security that are appropriate to the sensitivity of data. Confidentiality: Has documented capabilities for ensuring that administrative, technical, and physical safeguards are employed to comply with applicable confidentiality, risk management, and continuous monitoring requirements for sensitive data (should your data require such safeguards). Common Format: Allows datasets and metadata downloaded, accessed, or exported from the repository to be in widely used, preferably non-proprietary, formats consistent with those used in the community/communities the repository serves. Provenance: Has mechanisms in place to record the origin, chain of custody, and any modifications to submitted datasets and metadata. Retention Policy: Provides documentation on policies for data retention within the repository. 16.3 Considerations for Human Data When working with human participant data, including de-identified human data, here are some additional characteristics to look for: Fidelity to Consent: Uses documented procedures to restrict dataset access and use to those that are consistent with participant consent and changes in consent. Restricted Use Compliant: Uses documented procedures to communicate and enforce data use restrictions, such as preventing reidentification or redistribution to unauthorized users. Privacy: Implements and provides documentation of measures (for example, tiered access, credentialing of data users, security safeguards against potential breaches) to protect human subjects data from inappropriate access. Plan for Breach: Has security measures that include a response plan for detected data breaches. Download Control: Controls and audits access to and download of datasets (if download is permitted). Violations: Has procedures for addressing violations of terms-of-use by users and data mismanagement by the repository. Request Review: Makes use of an established and transparent process for reviewing data access requests. Data FAIRness Data that is Findable, Accessible, Interoperable, and Reusable. For more information about data FAIRness, check out this manuscript by Wilkinson et al. (2016). In brief, it is described by the NIH as follows: To be Findable, data must have unique identifiers, effectively labeling it within searchable resources. To be Accessible, data must be easily retrievable via open systems and effective and secure authentication and authorization procedures. To be Interoperable, data should “use and speak the same language” via use of standardized vocabularies. To be Reusable, data must be adequately described to a new user, have clear information about data-usage licenses, and have a traceable “owner’s manual,” or provenance. 16.4 No Existing Repository? What if I can’t find an appropriate repository? Supplemental material - If the data are small (less than 2 GB), it may be included as supplemental material for an article. See here for more information. Institutional repositories - Check to see if your institute has a repository where you could publicly share the data Generalist repositories- Host your data somewhere that hosts different types of data publicly, such as: Dataverse Dryad Figshare IEEE Dataport Mendeley Data Open Science Framework Synapse Vivli Zenodo Note that the NIH encourages that an existing data sharing repository be used whenever one is available instead of one of these options. 16.5 Data repositories The best way to share your data is by putting it somewhere that others can download it (and it can be kept private when necessary). There’s many repositories out there that handle this for you. Below are some of the standard repositories for data you should consider. For a longer list of repositories, we also advise consulting this Nature guidance on data repositories. 16.5.1 Genomic Data Repositories National Center for Biotechnology Information (NCBI) For microarray: GEO Gene Expression Omnibus (GEO) For RNA-seq: SRA (Sequencing Read Archive) European Molecular Biology Laboratory-European Bioinformatics Institute (EMBL-EBI) International Nucleotide Sequence Databases—DNA Data Bank of Japan (DDBJ) 16.5.2 Imaging data repositories Imaging data resource Cancer imaging archive 16.5.3 Repositories for journal articles For manuscripts or large datasets that are of atypical format, using one of these repositories is a good idea. The journal you submit to may have a recommendation of one over another. If not, you might end up having a preference. CyVerse Data Commons Repository Data Dryad FigShare ZENODO 16.5.4 Small datasets Data sets that are small and atypical format can be published as supplementary files as a part of a manuscript. 16.6 Finding a repository The following links can help you find a data repository for your data: Interactive table of NIH-supported data repositories Start here! Open NIH-supported domain-specific repositories Other NIH-supported domain-specific resources Nature data sharing resources Registry of research data repositories If you don’t find an appropriate repository for your data type: Consider adding your data as a supplementary file to a manuscript if it is small Consider an institutional repository Check out the generalist repositories References "],["data-submission-tips.html", "Chapter 17 Data Submission Tips 17.1 Health care data sharing tools", " Chapter 17 Data Submission Tips Uploading a dataset to a data repository is a great step toward sharing your data! But, the dataset uploaded is unclear and unusable it might as well not been uploaded in the first place. Keep in mind that although you may understand the ins and outs of your dataset and project, its likely that others who look at your data will not understand your notation. To make your data truly shared, you need to take the time to make sure it is well-organized and well-described! There are two files you should make sure to include to help describe and organize your data project: A main README file that orients others to what is included in your data. A metadata file that samples that are included, how they are connected, and when appropriate following privacy ethics, describes clinical features. Standards for genomic metadata 17.0.1 Use consistent and clear names Make sure that sample and data IDs used are consistent across the project - make sure to include a metadata file that describes in detail your samples in a way that is clear without any prior knowledge of the project. Sample and data IDs should keep with standard formatting otherwise known in the field. Features names should avoid using genomic coordinates as these may change with new genome versions. 17.0.2 Make your project reproducible Reproducible projects are able to be re-run by others to obtain the same results. The main requirements for a reproducible project are: The data can be freely obtained from a repository (this maybe summarized data for the purposes of data privacy). The code can be freely obtained from GitHub (or another similar repository). The software versions used to obtain the results are made clear by documentation or providing a Docker container. The code and data are well described and organized with a system that is consistent. 17.0.3 Have someone else review your code and data! The best way to find out if your data are useable by others is to have someone else look it over! There are so many little details that go into your data and projects. Those details can easily lead to typos and errors upon data submission and also can lead to confusion when others (or your future self) are attempting to use that data.The best way to test if your data project is usable is to have someone else (who has not prepared the data) is able to make sense of it. For more details on how to make data and code reproducible tips, see our Intro to Reproducibility course. 17.1 Health care data sharing tools 17.1.1 REDCap (Research Electronic Data Capture) REDCap is a very widely used browser-based software application for managing surveys and databases. It is very often used for clinical data. In fact, it is so widely used that there is a conference dedicated to it. REDCap allows for multi-institutional work, as well as compliance with HIPAA, 21 CFR Part 11 for data for the FDA, FISMA for government data, HIPAA, and GDPR for data for the European Union. It was developed by a team at Vanderbilt University in 2004. It is not open-source, however it is free to use for non-commercial research (redcap_2022?). You can find out more about how to use REDCap at the REDCap website which includes instructional videos and other resources. There are several things to keep in mind when using REDCap from an ethical standpoint. Roles REDCap allows for various roles to be established for users on a project. Thus access to certain data and tasks can be restricted to certain individuals. As described previously, it is a good idea to restrict access to the smallest number of individuals necessary. You can modify these roles using the User Rights menu. This will first show you who has what role on the project and their rights. You can click on an individual role to modify it. These roles should be verified by your institutional review board (IRB) before beginning a study. Changes to roles should also be reviewed by your IRB. Reports Reports that are exported can be customized to only show data that should be shared with the individual that you plan to share with. Please see the section on de-identification to better understand what data you might want to be restrictive about sharing. Again, the way you intend to share your data should be reviewed by your IRB before you begin your study. For example, you might remove the dates from the following report: Auditing REDCap keeps track of all data modifications, as well as data exports or report generations, in addition to keeping track of who performs those actions. This can be helpful for checking what has happened and when, in case anything happens that is unexpected or unintended. This is also great from a reproducibility or transparency standpoint - you have a record of any modifications to the data. This information can be obtained from the logging menu. Keep instruments short If your instruments are too long, this can result in accidentally sharing data that you don’t intend to, simply because you have more data to sift through. This also makes it easier to generate reports only on specific data that you would like to share. Data can be locked You can protect your data from accidentally being modified by locking specific data. Furthermore, at later stages of the project the data can no longer be modified. Keep in mind that your institution likely has their own guidelines for how to use REDCap should you decide to use it. Also remember to verify what you plan to do with your institutional review board (IRB) before you begin the study. "],["about-the-authors.html", "About the Authors", " About the Authors These credits are based on our course contributors table guidelines.     Credits Names Pedagogy Lead Content Instructor(s) Carrie Wright Candace Savonen Content Editor(s)/Reviewer(s) Nathan Boyd Technical Course Publishing Engineer(s) Candace Savonen Template Publishing Engineers Candace Savonen, Carrie Wright, Ava Hoffman Publishing Maintenance Engineer Candace Savonen Technical Publishing Stylists Carrie Wright, Ava Hoffman, Candace Savonen Package Developers (ottrpal) Candace Savonen, John Muschelli, Carrie Wright Art and Design Illustrator(s) Carrie Wright Candace Savonen Figure Artist(s) Carrie Wright Candace Savonen Funding Funder(s) NCI UE5CA254170 Funding Staff [Shasta Nicholson], Maleah O’Connor, [Sandy Ombrek]   ## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 4.3.2 (2023-10-31) ## os Ubuntu 22.04.4 LTS ## system x86_64, linux-gnu ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz Etc/UTC ## date 2025-10-06 ## pandoc 3.1.1 @ /usr/local/bin/ (via rmarkdown) ## ## ─ Packages ─────────────────────────────────────────────────────────────────── ## package * version date (UTC) lib source ## bookdown 0.43 2025-04-15 [1] CRAN (R 4.3.2) ## bslib 0.6.1 2023-11-28 [1] RSPM (R 4.3.0) ## cachem 1.0.8 2023-05-01 [1] RSPM (R 4.3.0) ## cli 3.6.2 2023-12-11 [1] RSPM (R 4.3.0) ## devtools 2.4.5 2022-10-11 [1] RSPM (R 4.3.0) ## digest 0.6.34 2024-01-11 [1] RSPM (R 4.3.0) ## ellipsis 0.3.2 2021-04-29 [1] RSPM (R 4.3.0) ## evaluate 1.0.4 2025-06-18 [1] CRAN (R 4.3.2) ## fastmap 1.1.1 2023-02-24 [1] RSPM (R 4.3.0) ## fs 1.6.3 2023-07-20 [1] RSPM (R 4.3.0) ## glue 1.7.0 2024-01-09 [1] RSPM (R 4.3.0) ## htmltools 0.5.7 2023-11-03 [1] RSPM (R 4.3.0) ## htmlwidgets 1.6.4 2023-12-06 [1] RSPM (R 4.3.0) ## httpuv 1.6.14 2024-01-26 [1] RSPM (R 4.3.0) ## jquerylib 0.1.4 2021-04-26 [1] RSPM (R 4.3.0) ## jsonlite 1.8.8 2023-12-04 [1] RSPM (R 4.3.0) ## knitr 1.50 2025-03-16 [1] CRAN (R 4.3.2) ## later 1.3.2 2023-12-06 [1] RSPM (R 4.3.0) ## lifecycle 1.0.4 2023-11-07 [1] RSPM (R 4.3.0) ## magrittr 2.0.3 2022-03-30 [1] RSPM (R 4.3.0) ## memoise 2.0.1 2021-11-26 [1] RSPM (R 4.3.0) ## mime 0.12 2021-09-28 [1] RSPM (R 4.3.0) ## miniUI 0.1.1.1 2018-05-18 [1] RSPM (R 4.3.0) ## pkgbuild 1.4.3 2023-12-10 [1] RSPM (R 4.3.0) ## pkgload 1.3.4 2024-01-16 [1] RSPM (R 4.3.0) ## profvis 0.3.8 2023-05-02 [1] RSPM (R 4.3.0) ## promises 1.2.1 2023-08-10 [1] RSPM (R 4.3.0) ## purrr 1.0.2 2023-08-10 [1] RSPM (R 4.3.0) ## R6 2.5.1 2021-08-19 [1] RSPM (R 4.3.0) ## Rcpp 1.0.12 2024-01-09 [1] RSPM (R 4.3.0) ## remotes 2.4.2.1 2023-07-18 [1] RSPM (R 4.3.0) ## rlang 1.1.6 2025-04-11 [1] CRAN (R 4.3.2) ## rmarkdown 2.25 2023-09-18 [1] RSPM (R 4.3.0) ## sass 0.4.8 2023-12-06 [1] RSPM (R 4.3.0) ## sessioninfo 1.2.2 2021-12-06 [1] RSPM (R 4.3.0) ## shiny 1.8.0 2023-11-17 [1] RSPM (R 4.3.0) ## stringi 1.8.3 2023-12-11 [1] RSPM (R 4.3.0) ## stringr 1.5.1 2023-11-14 [1] RSPM (R 4.3.0) ## urlchecker 1.0.1 2021-11-30 [1] RSPM (R 4.3.0) ## usethis 2.2.3 2024-02-19 [1] RSPM (R 4.3.0) ## vctrs 0.6.5 2023-12-01 [1] RSPM (R 4.3.0) ## xfun 0.52 2025-04-02 [1] CRAN (R 4.3.2) ## xtable 1.8-4 2019-04-21 [1] RSPM (R 4.3.0) ## yaml 2.3.10 2024-07-26 [1] CRAN (R 4.3.2) ## ## [1] /usr/local/lib/R/site-library ## [2] /usr/local/lib/R/library ## ## ────────────────────────────────────────────────────────────────────────────── "],["references.html", "Chapter 18 References", " Chapter 18 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
