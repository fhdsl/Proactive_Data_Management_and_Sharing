[["index.html", "Proactive Data Management and Sharing About this Course", " Proactive Data Management and Sharing October, 2025 About this Course This course is part of a series of courses for the Informatics Technology for Cancer Research (ITCR) called the Informatics Technology for Cancer Research Education Resource. This material was created by the ITCR Training Network (ITN) which is a collaborative effort of researchers around the United States to support cancer informatics and data science training through resources, technology, and events. This initiative is funded by the following grant: National Cancer Institute (NCI) UE5 CA254170. Our courses feature tools developed by ITCR Investigators and make it easier for principal investigators, scientists, and analysts to integrate cancer informatics into their workflows. Please see our website at www.itcrtraining.org for more information. "],["introduction.html", "Chapter 1 Introduction 1.1 Motivation 1.2 Target Audience 1.3 Topics covered 1.4 Curriculum", " Chapter 1 Introduction 1.1 Motivation The cancer research discipline has evolved into an increasingly complex mix of datasets - research projects are typically cross-disciplinary and contain many types of data in various formats. They often involve multiple collaborators generating data across different sites, with different data standards and infrastructure used to generate those data. Therefore, it is more important than ever to be well-versed in the best practices of data management and sharing. Proper data management and sharing is a necessity for cancer research projects to succeed in positively impacting cancer care. Now, it is more important than ever to understand the appropriate methods and best practices in data management and sharing as you plan for your research. The NIH and other cancer research funders have implemented mandates that require you to proactively plan to manage and share your data. As a member of the cancer research community, it is imperative that you maintain well-documented metadata and properly share your data. This will benefit you, your colleagues, and the larger community by broadening the reach of your data, enabling data reuse by others, and ultimately accelerating the pace of scientific discovery. This course aims to serve as a starting point to cover the basics of good data management and sharing practices. 1.2 Target Audience The course is intended for individuals in biomedical scientists and program managers who want to learn the best practices and techniques for data management and sharing. 1.3 Topics covered This course covers how to properly manage and share data including: 1.4 Curriculum How to use this course: This course contains high-level concepts for data management and sharing and can be used as a reference of suggested best practices and associated skills needed for data management and sharing in biomedical research. Keep in mind: Scientific data and research projects come in many different forms, and some content in this course may not apply, especially as the research landscape evolves to adapt and support new technology, methods, and techniques. Therefore, the goal of this course is not to prescribe rigid rules for how to conduct research, but rather serve as a guide to approach data management and sharing in the spirit of the FAIR principles (Findable, Accessible, Interoperable, Reusable). We encourage you to continue to consult with data management experts to suit the needs of your particular project and/or research goals. Disclaimer: This course material is for instructional use only and is not a substitute for legal or ethical advice. The findings and conclusions in this course are those of the authors and do not represent official guidance from the National Institutes of Health. "],["data-sharing-is-important.html", "Chapter 2 Data Sharing is Important", " Chapter 2 Data Sharing is Important Sharing data is critical for optimizing the advancement of scientific understanding. Now that labs all over the world are producing massive amounts of data, there are many discoveries that can be made by simply re-using/re-analyzing this existing data. The concept of data re-use is so important, that in January 2023, after years of development, the NIH released new guidance and began requiring new practices for data management and sharing. See the announcement here. See this course about the NIH policy for more information about how to comply. Note that many institutes and funding agencies or mechanisms have requirements about how your data can be shared. Typically, data sharing of protected data also requires Institutional Review Board (IRB) approval before the study is conducted. Ensure that you are following those requirements before you share your data. A later section in this course will cover data privacy. There’s so many excellent reasons to put your data in a repository, whether or not a journal requires it: Sharing your data… Makes your project more transparent and thus more likely to be trusted and cited. In fact, one study found that articles with links to the data used (in a repository) were cited more than articles without such information or other forms of data sharing (Colavizza et al. 2020). Helps reduce your own workload so your email inbox isn’t overloaded by requests you probably don’t have time to respond to. Allows others to gain even more insights from your data which shows funders that your data will be used to its maximum potential. In summary, data sharing is the practice of making all appropriate data sources associated with a project available through systematic means. Broad data sharing is a critical step toward mitigating the reproducibility and replicability crisis. Not only that, but data sharing is economical, equitable, and frugal. New exciting findings can result from shared and reused data. Additionally, institutions and individuals with less resource availability can benefit from access to shared data. References "],["defining-reproducibility.html", "Chapter 3 Defining reproducibility 3.1 Learning Objectives 3.2 What is reproducibility 3.3 Reproducibility in daily life 3.4 Reproducibility is worth the effort! 3.5 Reproducibility exists on a continuum!", " Chapter 3 Defining reproducibility 3.1 Learning Objectives 3.2 What is reproducibility There’s been much discussion about what is included in the term reproducibility and there is some discrepancy between fields. For the broad field of cancer research, a reproducible analysis is one that can be re-run by a different researcher and the same result and conclusion is found. Reproducibility is related to repeatability and replicability but it is worth taking time to differentiate these terms Perhaps you are like Ruby and have just found an interesting pattern through your data analysis! This has probably been the result of many months or years on your project and it’s worth celebrating! But before she considers these results a done deal, Ruby should test whether she is able to re-run her own analysis and get the same results again. This is known as repeatability. Given that Ruby’s analysis is repeatable; she may feel confident now to share her preliminary results with her colleague, Avi the Associate. Whether or not someone else will be able to take Ruby’s code and data, re-run the analysis and obtain the same results is known as reproducibility. The same could be true for running the same experimental protocol with the same samples by another colleague. If Ruby’s results are able to be reproduced by Avi, now Avi may collect new data and use Ruby’s same analysis methods to analyze his data (or collect new samples and run the same experimental protocol). Whether or not Avi’s new data/samples and results concur with Ruby’s study’s original inferences is known as replicability. You may realize that these levels of research build on each other (like science is supposed to do). In this way, we can think of these in a hierarchy. Skipping any of these levels of research applicability can lead to unreliable results and conclusions. Science progresses when data and hypotheses are put through these levels thoroughly and sequentially. If results are not repeatable, they won’t be reproducible or replicable. Ideally all analyses and results would be reproducible without too much time and effort spent; this would aid in the efficiency of research getting to the next stages and questions. But unfortunately, in practice, reproducibility is not as commonplace as we would hope. Institutions and reward systems generally do not prioritize or even measure reproducibility practices. Standards in research and training opportunities for reproducible techniques can be scarce. Reproducible research can often feel like an uphill battle that is made steeper by lack of training opportunities. In this course, we hope to equip your research with the tools you need to enhance the reproducibility of your analyses so this uphill battle is less steep. 3.3 Reproducibility in daily life What does reproducibility mean in the daily life of a researcher? Let’s say Ruby’s results are repeatable in her own hands and she excitedly tells her associate, Avi, about her preliminary findings. Avi is very excited about these results as well as Ruby’s methods! Avi is also interested in Ruby’s analysis methods and results. So, Ruby sends Avi the code, data, and methods she used to obtain the results. Now, whether or not Avi is able to obtain the same exact results with this same data and same analysis code will indicate if Ruby’s analysis is reproducible. Ruby may have spent a lot of time on her code and getting it to work on her computer, but whether it will successfully run on Avi’s computer is another story. Often when researchers share their analysis code it leads to a substantial amount of effort on the part of the researcher who has received the code to get it working and this often cannot be done successfully without help from the original code author (Beaulieu-Jones and Greene 2017). This same concept applies to experimental research methods in a laboratory setting. Avi is encountering errors because Ruby’s code was written with Ruby’s computer and local setup in mind and she didn’t know how to make it more generally applicable. Avi is spending a lot of time just trying to re-run Ruby’s same analysis on her same data; he has yet to be able to try the code on any additional data (which will likely bring up even more errors). Imagine a trying to follow an experimental research method in the lab with vague or unclear instructions! Avi is still struggling to work with Ruby’s code and is confused about the goals and approaches the code is taking. After struggling with Avi’s code for an untold amount of time, Avi may decide it’s time to email Ruby to get some clarity. Now both Avi and Ruby are confused about why this analysis isn’t nicely re-running for Avi. Their attempts to communicate about the code through email haven’t helped them clarify anything. Multiple versions of the code may have been sent back and forth between them and now things are taking a lot more time than either of them expected. Perhaps at some point Avi is able to successfully run Ruby’s code on Ruby’s same data. Just because Avi didn’t get any errors doesn’t mean that the code ran exactly the same as it did for Ruby. Lack of errors also doesn’t mean that either Ruby or Avi’s runs of the code ran with high accuracy or that the results can be trusted. Even a small difference in decimal point may indicate a more fundamental difference in how the analysis was performed and this could be due to differences in software versions, settings, or any number of items in their computing environments. This challenge also exists when trying to repeat a research method that you or someone else has written, especially if there aren’t enough details to precisely describe the conditions in which the data was collected. 3.4 Reproducibility is worth the effort! Perhaps you’ve found yourself in a situation like Ruby and Avi; struggling to re-run code or a method that you thought for sure was working a minute ago. In the upcoming chapters, we will discuss how to bolster your projects’ reproducibility. As you apply these reproducible techniques to your own projects, you may feel like it is taking more time to reach endpoints, but keep in mind that reproducible analyses and projects have higher upfront costs but these will absolutely pay off in the long term. Reproducibility in your analyses is not only a time saver for yourself, but also your colleagues, your field, and your future self! You might not change a single character in your code or a step in your method but then return to it in a few days/months/years and find that it no longer runs! Reproducible code and research methods stands the test of time longer, making ‘future you’ glad you spent the time to work on it. It’s said that your closest collaborator is you from 6 months ago but you don’t reply to email (Broman 2016). Many a researcher has referred to their frustration with their past selves: Dear past-Hadley: PLEASE COMMENT YOUR CODE BETTER. Love present-Hadley — Hadley Wickham (@hadleywickham) April 7, 2016 The more you comment your code, or detail your method and make it clear and readable, your future self will thank you. Reproducible code and research protocols also saves your colleagues time! The more reproducible your methods are, the less time your collaborators will need to spend troubleshooting it. The more people who use your methods need to try to troubleshoot it, the more time is wasted. This can add up to a lot of wasted time and effort. But, reproducible code and methods saves everyone exponential amounts of time and effort! It will also motivate individuals to use and cite your methods in the future! 3.5 Reproducibility exists on a continuum! Incremental work on your analyses is good! You do not need to make your analyses perfect on the first try or even within a particular time frame. The first step in creating an analysis is to get it to work once! But the work does not end there. Furthermore, no analysis is or will ever be perfect in that it will not be reproducible in every single context throughout time. Incrementally pushing our analyses toward the right of this continuum is the goal. References "],["data-privacy.html", "Chapter 4 Data Privacy 4.1 PII (personal identifiable information) 4.2 PHI (protected health information) 4.3 PHI is a subset of PII 4.4 PHI Risk 4.5 Cancer research data and PHI 4.6 How to ensure the privacy of this information 4.7 How is HIPAA enforced?", " Chapter 4 Data Privacy Cancer research often involves personal health data that requires compliance with Health Insurance Portability and Accountability Act (HIPAA) regulations. In this chapter we will discuss data management strategies to maintain compliance with these important regulations. Cancer research often involves the collection of information about research participants that is personal. There are two categories of such information: personal identifiable information (PII) and protected health information (PHI) Note that these are general definitions and whether something counts as PII or PHI has to be evaluated in a case-by-case basis. 4.1 PII (personal identifiable information) PII (personal identifiable information) are aspects of a person that could allow you to identify that person. PII is defined by the US Department of Labor as: “Any representation of information that permits the identity of an individual to whom the information applies to be reasonably inferred by either direct or indirect means.” PII is also defined by the US General Services Administration as: “Information that can be used to distinguish or trace an individual’s identity, either alone or when combined with other personal or identifying information that is linked or linkable to a specific individual.” Why is this term defined by the Department of Labor and the US general Services Administration? Because the Privacy Act of 1974 (privacy_act_2022?), is a US federal law that governs the “collection, maintenance, use and dissemination” of personal information. US agencies have access to a large amount of PII and must act in accordance with the Privacy Act to protect this data. Examples include (but aren’t limited to): Name Telephone number Address Social security number Age Driver’s licenses Medical record numbers Full face photographs IP addresses Some PII as in the examples above can pose significant risk to individuals if other people were to gain access, like social security numbers. Other PII, like age, does not necessarily pose as much risk unless combined with other information. Thus, this information is categorized in two ways: nonsensitive, which is easy to find and poses little risk, and sensitive information, which is harder to find, poses higher risk, and requires more protection. 4.1.1 PII Risk What is the risk of PII getting into the hands of people it shouldn’t? Why was the Privacy Act necessary? PII can pose a risk for identity theft, which can have financial, professional, criminal, and personal consequences (dinardi_14_2022?), as criminals can get loans and credit card in other people’s names, as well as commit crimes under the guise of other people’s identities. This can result in reputation loss and loss of opportunities. In addition, the leak of PII can also pose a safety risk, as criminals can identify the likely locations of specific individuals if performing targeted crimes. 4.2 PHI (protected health information) The U.S. Department of Health &amp; Human Services describes protected health information (PHI) as: …information including demographic data that relates to: the individual’s past, present or future physical or mental health or condition, the provision of health care to the individual, or the past, present, or future payment for the provision of health care to the individual This includes 18 categories: Patient names Geographical elements (such as a street address, city, county, or zip code) Dates related to the health or identity of individuals (including birthdates, date of admission, date of discharge, date of death, or exact age of a patient older than 89) Telephone numbers Fax numbers Email addresses Social security numbers Medical record numbers Health insurance beneficiary numbers Account numbers Certificate/license numbers Vehicle identifiers Device attributes or serial numbers Digital identifiers, such as website URLs IP addresses Biometric elements, including finger, retinal, and voiceprints Full face photographic images Other identifying numbers or codes 4.3 PHI is a subset of PII PHI is a subset of PII. It is personal identifiable information that relates to or could relate to an individual’s health. Some PII is always PHI, like health insurance numbers or clinical data such as radiology reports with names or other distinguishing features. Other PII becomes PHI based on context. For example, name and email address aren’t necessarily PHI, unless they are in the context of medical care or research. This could be the case if a patient receives notes from the doctor through email or researchers have a database of participants with email addresses that could be used to distinguish the identity of people in the study. 4.4 PHI Risk PHI poses an additional risk rather than just typical PII because it includes sensitive health information. This can be used to determine if an individual has a particular condition or health risk and could be misused in employment or insurance decisions. 4.5 Cancer research data and PHI Certain omics or genomics data, such as whole genome sequencing (essentially a genomic signature), and some radiology images with distinguishing features can be used to identify individuals. Advances in machine learning may further increase the identifiability of these data types in the future. 4.5.1 What genomic data is protected? So what does this mean for the data you handle? A non-comprehensive list of identifiable and protected information: Some clinical information in metadata (should be carefully reviewed and de-identified where possible) Genomic sequences Whole genome sequences Exome sequencing Whole transcriptome sequencing Single nucleotide polymorphisms Genealogy information What is not protected and generally is safe: Summarized cohort data Data in which individuals have been aggregated together is generally safe. For example, a file that includes an average age calculated across all individuals or a large subset would generally be considered safe. However, this may not always be the case with individuals with very rare conditions or individuals belonging to a small group (such as indigenous or pediatric populations). De-identified data Data where all personal identifiers that could link the data to a specific individual are removed, making the data anonymous and safe for sharing under certain conditions. However, in the context of genomic data, de-identification may not always guarantee complete anonymity. This is because genomic data, especially when it contains rare or unique variants, can sometimes be linked back to individuals. The presence of such variants may allow re-identification, particularly if the data is combined with external datasets. As a result, additional protections may be necessary, such as restricted access or data sharing with safeguards in place, to prevent re-identification risks. It has been shown that certain types of de-identified genomic data can be re-identified due to the availability of genomic data in datasets like 23andMe, where relatives with unique genomic features can be used to identify relatives of individuals in studies. The following articles have more extensive information about the current re-identification risk of different genomic data types: Privacy considerations for sharing genomics data Identifying personal genomes by surname inference Preserving genomic privacy via selective Sharing Impact of HIPAA’s minimum necessary standard on genomic data sharing Genetic information privacy The law and medical privacy The broken promise that undermines human genome research 4.6 How to ensure the privacy of this information Your institution will have guidance on how to protect sensitive data but in general there are 4 main strategies we will summarize here: Limit access to the data The protected data is seen by the smallest number of individuals possible, all of whom have been properly trained and certified to handle data. Make sure the data are stored in a place that only these few people who are allowed have access to it. If you aren’t sure who has access to a place – don’t put the data there! Aggressively de-identify the shared data Before results or data are shared or published, they must be de-identified. We will discuss more about what this is in the next chapter. If data have been summarized at the cohort-level with no personal identifiers, then it is probably safe to share. Consider a data use agreement A Data Use Agreement (DUA) is required even for de-identified data, particularly when human subjects data are shared for research purposes or across institutions. While HIPAA does not mandate a DUA for fully de-identified data under certain conditions, other factors—such as institutional policies, ethical concerns, or specific data sharing agreements—may require one. Data use agreements restrict who can access and use the data that you might share, as well as what they may do with the data. Importantly this needs to be agreed upon by an IRB and consented to by the research participants in some manner before it is in use. See this guidance on DUAs and this practices guide for DUAs for more information about when you might need a data use agreement. Note that your particular situation and institute may have slightly different rules or restrictions. Check out these DUA templates to get started: DUA template from the Harvard Catalyst The NIH uses this certification agreement. Be sure to follow the attribution guidelines outlined in the links if you adapt them for your use. When in doubt, prioritize caution If you are uncertain whether data contains PHI or PII, consult with relevant offices at your institute, such as an IRB, a research administration office, or a HIPAA compliance office. If you plan to share your data somewhere and you are unsure whether a database or repository is secure and HIPAA compliant, ask those who manage that database or repository to confirm! 4.7 How is HIPAA enforced? The Office for Civil Right (OCR) of the United States Department of Health and Human Services is in charge of enforcing HIPAA compliance. If you feel that someone is using or sharing data that are in violation of HIPAA compliance, in most cases, you should start by attempting to resolve the violation first through local means by contacting research administrators or management. However, you can also choose to file a complaint online using the OCR compliant portal. Note that complaints should be filed within 180 days of the violation. If the OCR determines that a covered entity is in violation (the individuals or institutes who are required to follow HIPAA compliance regulations), then the OCR will follow up to ensure that the entity complies, takes corrective action, or agrees to a settlement. What is a covered entity? These are health care providers (doctors, dentists etc.), health plan/insurance companies and programs (Medicare, Medicaid, etc.) and those who work at health care clearinghouses (places that process nonstandard health information). See this link about covered entities for more information and see this link website for more in-depth information. If compliance is not resolved, then the covered entity may have to pay fines. The civil fines for HIPAA violations by a covered entity range from $100 to $50,000 PER VIOLATION for willful neglect and untimely correction, with a $1.5M yearly cap on fines. In addition to civil penalties, an individual who knowingly committed the violation may face a criminal penalty of up to $50,000 and 1 year in prison. If the case involves misrepresentation/misleading conduct, penalties increase to $100,000 and 5 years prison, and to $250,000 and 10 years prison if the violation involves personal gain or malicious harm (violations_2018?). 4.7.1 Common Violations Data security and HIPAA violations are talked about more in this course about ethical data handling. It offers more information to mitigate the following challenges. Common violations of HIPAA taken from (violations_2018?) are: A lack of encryption If your email or data transfer is intercepted it is important to keep your data safe! Computer hacking or phishing If your computer gets hacked by hackers through a phishing email or otherwise, they could sell the data to third party organizations who could profit off of the information. The data security practices that we will describe in the next chapter will help avoid this. Unauthorized Access Allowing or accidentally allowing fellow lab mates who are not authorized to access the data is a violation of HIPAA. This can lead to other neglectful or malicious practices that result in larger disclosures of PHI. Furthermore, using your laptop in public or even at home can pose a risk from people who may walk by. Loss or Theft of Devices If your laptop or external storage device is stolen, data files with PHI can easily be obtained by whoever finds them next. Improper Disposal of data or devices Sometimes there are remnants of your data still on your device! Unsecured access to data Accessing your data from an unsecured WIFI network can also make the data vulnerable. See the Privacy Rule and research FAQs for more information about HIPAA and research. "],["creating-clear-documentation.html", "Chapter 5 Creating Clear Documentation 5.1 Characteristics of clear documentation 5.2 It is comprehensive 5.3 Types of documentation you should have", " Chapter 5 Creating Clear Documentation Our goal for documentation is to be as comprehensive, navigable, and as always, as clear as possible. 5.1 Characteristics of clear documentation The following are tips to make your documentation as useful as possible. 5.1.1 It is easy to find No matter how well your documentation is crafted, it is of no use if no one can find them. Having documents that are standard and at the top of your directory is key. READMEs for example are standard documents in software that give the TL;DR of the project. In science, READMEs are incredibly valuable 5.2 It is comprehensive All items are covered in the documentation in an organized fashion – every. single. thing. This includes all: Data sources and versions Metadata Software dependencies and their versions Terms Functions Arguments Parameters Defaults The most useful documentation… Not only define the items and files included, but tells how it relates to other items (and they have links where relevant). Make any existing defaults and calculations very clear. It doesn’t assume that just because a term is used, the calculation is obvious. For example, Tumor Mutation Burden is a common statistic to report but it is calculated different ways. Documentation should describe major calculations and not assume standardization. Shows how to re-run the entire experimental protocol or analysis, example lines of code go a long way. Tries to avoid the use of jargon, but if it is absolutely necessary to use a jargon-y term it links to information about the meaning of the term. 5.2.1 Data formats are described Perhaps after installation, getting data formatted correctly is one of the other very large hurdles users will need to deal with. Ideally, your software or analysis can use a data format that is common. But the more that your protocol or tool is particular about an odd data format, the more your documentation needs to be specific about what the odd data format looks like. It’s very helpful to include subsetted, de-identified example files for a positive control/example. 5.3 Types of documentation you should have Documentation strategies are not one size fits all but there are two types of documentation we strongly advise every project has: READMEs and analysis notebooks. We refer you to see the OpenPBTA project as a real life example of well documented open source data analysis. 5.3.1 READMEs! READMEs are also a great way to help your collaborators get quickly acquainted with the project. READMEs stick out in a project and are generally universal signal for new people to the project to start by READing them. For code, GitHub automatically will preview your file called “README.md” when someone comes to the main page of your repository which further encourages people looking at your project to read the information in your README. Information that should be included in a README: General purpose of the project Instructions on how to re-run the project Lists of any software required by the project Input and output file descriptions Descriptions of any additional tools included in the project You can take a look at this template README to get your started. 5.3.1.1 More about writing READMEs: How to write a good README file How to write an awesome README 5.3.2 Exercise: Write a README for your project! Download this template README. Fill in the questions inside the { } to create a README for this project. See the R README template and the Python README template for more help if you are writing code for your project. Add your README to your GitHub repository. Follow these instructions to add files to GitHub repositories. 5.3.3 12.2.3 Notebooks The generous use and keeping of notebooks is a useful tool for documentation of the development of an analysis or research project. Data analyses can lead one on a winding trail of decisions and side investigations, but notebooks allow you to narrate your thought process as you travel along these analyses explorations! Your scientific notebook should include descriptions that describe: 5.3.4 The purposes of the notebook What scientific question are you trying to answer? Describe the dataset (or samples) you are using to try to answer this and why. 5.3.5 The reasons behind your decisions Describe any unconventional method steps or anything that might be forgotten or confusing later or to someone not involved in the study. Note this can be for experimental work or computational data analysis. For example: Describe why a particular code chunk is doing a particular thing – the more odd the code looks, the greater need for you to describe why you are doing it. Describe any particular filters or cutoffs you are using and how did you decide on those? For data wrangling steps, why are you wrangling the data in such a way – is this because a certain package you are using requires it? 5.3.6 Your observations of the results What do you think about the results? The plots and tables and other visualizations and results that you show in the notebook – how do they inform your original questions? See this course about reproducible workflows in R for more information about the use of Notebooks to better inform others about your work! "],["why-documentation-is-worth-the-time.html", "Chapter 6 Why Documentation is Worth the Time 6.1 The context of documentation in research 6.2 Why documentation is worth the time", " Chapter 6 Why Documentation is Worth the Time 6.1 The context of documentation in research Biomedical research comes in all shapes and sizes, varying from mostly experimental wet bench work to a combination of experimental and computational, to largely computational. Many researchers don’t realize the work needed to document computational work. These activities could include: Scripts Workflows Pipelines Algorithms and computational methods Often projects may start with one person developing and using the computational work, but this may expand to other lab members, collaborators, and others in the broader scientific community. But many researchers don’t have a computer science background and many are self-taught and may not realize what is needed to document their process. Research, whether code is involved or not, is an exciting but long process – filled with side investigations, tedious troubleshooting, but also ‘Aha’ moments that ultimately can result in amazing results that you should be proud of! The code and the methods you use are likely valuable to more than just the singular project you made it for. Indeed, others may have needs for the methods you use and will be excited to come across your code and tools! Other researchers are likely eager to apply your code and methods to their own work but its unfortunately all too common that scientific code is not able to be reused. Even scientists who are skilled with analysis often struggle to make work reproducible. In a large-scale study, only 24% of scientific notebooks ran without errors and only 4.03% produced the same results. There is a great need for reproducible work and a large part of reproducibility is clear and findable documentation! Open source code is a valuable practice for contributing to the scientific community but if the code lacks clear documentation it is incomplete. Undocumented code can lead to a lot of frustration and time inefficiently spent. If a code base’s documentation is non-existent, scarce, out-of-date, or filled with too much jargon, the chances are high that no one will be able to successfully and efficiently re-use this work, despite their needs to do so. Lack of usability often leads researchers to ditch even the most well-programmed of tools and code. This is the unfortunate and all-too-common result of many bioinformatics tools. 6.2 Why documentation is worth the time We realize many researchers feel unenthused about the process of creating documentation or may lack bandwidth to do so. They may know it’s good for their research, but they just aren’t enthused about it. We’d like to assure you that the effort for creating documentation has a high return payoff for the continued success of your research code/scientific software as a whole! Other researchers are still likely to encounter errors and problems, but with thorough and easy-to-digest documentation, they are better equipped to troubleshoot these problems! They may also learn more about the features and limitations of the code that will better guide their next steps! This is not only helpful for other researchers but makes it more likely that more individuals in the community will use these methods and share them in the community. These types of citations and usage metrics can be valuable to report to funding institutions to describe the impact of the work. Well-documented code helps developers better maintain their code in the future because they may forget the mechanics of their code over time. This helps with manuscript revisions, transparency or future research that builds on these methods! "],["discussion-on-benefits-of-data-management-and-sharing.html", "Chapter 7 Discussion on Benefits of Data Management and Sharing 7.1 Discussion", " Chapter 7 Discussion on Benefits of Data Management and Sharing 7.1 Discussion Please see below for a self-directed reflection. Before beginning this course, you may not have thought much about data management and data sharing as it relates to your project. If you are or have written a grant proposal, you understand that writing grants takes time and can be a lot of work. Before this course, you may have thought that adding an additional plan to your proposal is just more paperwork. With what you have learned so far in this course, . If applicable,consider some ways that sharing your data and managing it effectively might benefit you, your fellow cancer researchers, and the public in the long term. "],["why-is-there-an-nih-dms-policy.html", "Chapter 8 Why is there an NIH DMS policy? 8.1 Key terms 8.2 Grant Mechanisms 8.3 Grant Renewals 8.4 Impact on Reviews 8.5 Sharing Timeline 8.6 When to Not Share Data", " Chapter 8 Why is there an NIH DMS policy? First we will discuss the motivations behind the new data management and sharing policy that went into effect for (most) grant proposals submitted to the NIH after January 25, 2023. Why is the NIH doing this? There are several reasons why sharing data can be beneficial to the scientific community. Supports transparency - Sharing data provides more clarity about how studies are performed. Many scientists also believe in an ethical responsibility to study participants (Bauchner, Golub, and Fontanarosa 2016). Encourages reproducibility and rigor - Having the data accessible, allows others to try to reproduce study findings. This can further enable studies that may replicate or validate the initial findings with different data. Supports multi-modal work - When more data of various types are easily available it makes it easier for scientist to perform studies with multiple types of data (Thessen 2021). More efficient and cost effective - Some data are especially difficult or expensive to produce.By sharing this data more broadly it can save on cost and efficiency of designing new and similar studies by learning from the data already generated. Supports Researcher Inclusion - Data generation can be especially difficult for those at institutes with less resources. Publicly available data can therefore be used by these researchers to better enable their participation. Increased impact - Papers that share their data in repositories appear to be cited more based on the study by Colavizza et al. (2020) . Increased collaboration opportunity - Having data available can encourage other researchers to expand the research in a new direction or extend it further and they may reach out to collaborate. Data Citations - Due to the importance of data generation and sharing to the NIH, the culture will shift to see data as a research product that demonstrates a contribution to the scientific community. 8.1 Key terms Data Management The work involved with validating, organizing, protecting, maintaining, and processing scientific data to ensure the accessibility, reliability, and quality of data for use in research. All research data should be actively managed. Data Sharing The act of making scientific data available for use by others (e.g., the larger research community, institutions, the broader public), for example, via an established repository. Some data carry limitations on how data sharing can be done and some meet criteria that make them exempt from data sharing. Metadata Data that provide additional information intended to make scientific data interpretable and reusable. Metadata can include features like dates, independent sample and variable construction and description, methodology, data provenance, data transformations, any intermediate or descriptive observational variables. Data Management and Sharing Plan A plan describing an approach to data management, preservation, and sharing of scientific data and accompanying metadata. #How will this policy affect me? Whether you are an investigator applying for a NIH grant, or a researcher or trainee supported by a NIH grant, it is important to know how this policy applies to you and your work. You can refer to the NIH overview of which research will be covered by this new policy. The major requirement of the policy is that all grant proposals (submitted after January 25th, 2023) for mechanisms that require compliance, must include a plan for how they will proactively manage and share their data. For certain grant mechanisms for projects that do not generate data, compliance with the policy is not required. For certain types of data, sharing is not possible, and a justification will be required instead. The following text will discuss several key questions: Is my research exempt from the policy? Does my research generate scientific data? Do grant renewals require compliance with the policy? How will the policy impact the review process? When do I need to share my data? When should I not share data? To determine if your research requires compliance with other policies that may influence how you share your data, take this quiz. In addition to these questions, there are ethical considerations that you may want to think about. See the ethics section of our other course for more information. 8.2 Grant Mechanisms What grant mechanisms require compliance with the DMS policy? The DMS Policy applies to all research that generates scientific data (regardless of the funding level), including: Research Projects Some Career Development Awards (K) Small Business SBIR/STTR Research Centers The DMS Policy does not apply to research and other activities that do not generate scientific data, including: Training (T) Fellowships (F) Construction (C06) Conference Grants (R13) Resource (G) You can look up the [NIH Activity Code in this table] to see if a DMS Plan is required](https://sharing.nih.gov/sites/default/files/flmngr/List-of-Activity-Codes-Applicable-to-DMS-Policy.pdf){target=“_blank”} for your particular grant type. For example, I am interested in applying to a R03 award. According to the table, a DMS Plan is required for this particular award. 8.2.1 Data-generating Research Does my research generate scientific data? The NIH Data Management and Sharing (DMS) Policy applies to all NIH-supported research generating scientific data. But what is “scientific data”? 8.2.1.1 Scientific data Scientific data are the “recorded factual material of sufficient quality to validate and replicate research findings, regardless of whether the data are used to support scholarly publications”. This can include any of the following if they are applicable to your study: Unpublished results Null results Results used to publish papers 8.2.1.2 Not scientific data You are not expected to share: lab notebooks preliminary analyses case report forms drafts of scientific papers plans for future research peer reviews communications with colleagues physical objects (such as biospecimens) 8.3 Grant Renewals Do grant renewals need to comply with the policy? If you submit a grant renewal application for any of the grants mechanisms that require compliance after January 25th, 2023, then your renewal will need to include a DMS Plan even if the grant was originally funded before January 25th, 2023. 8.4 Impact on Reviews How will this influence the grant review process? For most proposals – those where data sharing is not part of the Notice of Funding Opportunity – the following will happen during the review process: Reviewers for will not have access to your DMS plan. Reviewers will however see your budget which will include some descriptions of how money will be spent to manage and share data. Thus, the DMS plan should not influence your grant score. After a grant receives a fundable score, a Program Officer will review the DMS plan and will work with the PI to address any concerns. Changes based on this process can be made during the Just-in-Time procedures. For proposals where data sharing is specified as part of the Notice of Funding Opportunity the following will happen during the review process: The reviewers will have access to the plan and it may be part of the review criteria. Program staff will also review the DMS plan. 8.5 Sharing Timeline When do the data need to be shared by? Data should be made available no later than publication or end of the award. This means that data underlying findings that are not published in peer-reviewed journals should be made available by the end of the award. 8.5.1 No-cost Extensions Scientific data should be made accessible as soon as possible, and no later than the time of an associated publication or the end of the performance period of the extramural award that generated the data. If a no cost extension is granted for an extramural award, scientific data should be made accessible no later than the time of an associated publication, or the end of the no cost extension, whichever comes first. 8.6 When to Not Share Data Under the NIH DMS Policy, is it possible to not share data? There are legitimate reasons you might not share your data. Data might not be shareable due to ethical, legal, or technical concerns. You will still need to submit a DMS Plan even if you plan to withhold data sharing. You must explain your reasoning in your DMS Plan. Justifiable ethical, legal, and technical factors for limiting sharing of data include: Informed consent will not permit or will limit the scope or extent of sharing and future research use Existing consent (e.g., for previously collected biospecimens) prohibits sharing or limits the scope or extent of sharing and future research use Privacy or safety of research participants would be compromised or place them at greater risk of re-identification or suffering harm, and protective measures such as de-identification and Certificates of Confidentiality would be insufficient. Explicit federal, state, local, or Tribal law, regulation, or policy prohibits disclosure Restrictions are imposed by existing or anticipated agreements with other parties Datasets cannot practically be digitized with reasonable efforts For additional information about potential ethical considerations, including if data is of sufficient quality to be shared. See the ethics section of our other course for more information. What if my data are proprietary? (click for more information) Considerations for Proprietary Data NIH understands that some scientific data generated with NIH funds may be proprietary. Under the Small Business Innovation Research (SBIR) and Small Business Technology Transfer (STTR) Program Policy Directive, effective May 2, 2019, SBIR and STTR awardees may withhold applicable data for 20 years after the award date, as stipulated in the specific SBIR/STTR funding agreement and consistent with achieving program goals. SBIR and STTR awardees are expected to submit a Data Management &amp; Sharing Plan per DMS Policy requirements. Issues related to proprietary data also can arise when co-funding is provided by the private sector (for example, the pharmaceutical or biotechnology industries). NIH recognizes that the extent of data sharing may be limited by restrictions imposed by licensing limitations attached to materials needed to conduct the research. Applicants should discuss projects with proposed collaborators early to avoid agreements that prohibit or unnecessarily restrict data sharing. NIH staff will evaluate the justifications of investigators who believe that they are unable to share data. For questions or concerns about data sharing expectations for proprietary data, please contact the NCI Office of Data Sharing or the (NCI Technology Transfer Center](https://techtransfer.cancer.gov/about/contact). Small businesses may wish to contact the NIH SBIR Development Center. The following are NOT good reasons to limit the sharing of your data: Data are considered too small Researchers anticipate data will not be widely used Data are not thought to have a suitable repository You don’t have the right personnel to manage data or share data You don’t want to pay for data storage References "],["elements-of-the-dms-plan.html", "Chapter 9 Elements of the DMS Plan 9.1 Overall Takeaways", " Chapter 9 Elements of the DMS Plan The NIH has provided an outline for what components of the Data Sharing and Management Plan are required. The following are the major elements required to be included in an NIH DMS plan: Data type - describe what data (amount and type) will be generated over the course of funding and what data will or will not be shared Tools, software, and code - describe what tools (and versions) you intend to use to manage and analyze the data (note code is not required to be shared) Standards - describe any standards that you are indeed to use for your data and metadata to be usable by others or to be contributed to a repository Data preservation, access, timelines - describe where the data will be made available and when Access, distribution, reuse considerations - describe how you have carefully considered any reasons that might limit sharing Oversight - describe who will manage compliance of the DMS plan You can find more detailed requirements for each of these elements on the NIH website. To aid in proactively planning for data management and sharing over the life of your research project, it may be helpful to consider the data types and size you plan to generate, the repositories available, and the corresponding budgetary implications before you begin your research. 9.1 Overall Takeaways Why this change? As of January 25, 2023, the NIH is requiring all grant proposals include a Data Management and Sharing Plan to aid in the transparency and reproducibility of NIH-funded research. Does this apply to me? This will apply to most NIH grants that create data (regardless of funding level), although some grant mechanisms and some data are exempt. It is not required to share all data, however a justification is required. Reasons not to share that are acceptable may be ethical, technical, or legal. Where should I share my data? The NIH has lists of suggested repositories, to help you find appropriate repositories for your data. If there is not an appropriate repository, you can share your data at an institutional (if available) or generalist repository. How do I budget this? You may request funds for data management and sharing as direct costs (including personnel costs). Infrastructure costs should only be included as indirect costs. How do I write my plan? Your plan should be &lt; 2 pages (without hyperlinks) and include sections on data type; tools, software, and code; standards; data preservation, access, and timelines; access, distribution, and reuse considerations; and oversight. You may be able to change your plan in the just-in-time process or during regular reporting intervals. Changes require approval. Will my plan influence my grant score? Although the Data Management and Sharing Plan is a mandatory part of most grant proposals, it will not be shared with reviewers and thus will not influence your score, however the budget will be visible to reviewers. Final Considerations This is a new policy for the NIH and they are expecting to have some growing pains. While they don’t currently have specific expectations, the NIH has stated they will learn from the process as it happens along with the scientists submitting grant proposals under the new policies. In general, it’s best to be mindful of what is actually feasible when it comes to managing and storing your data and try not overstate what you might be able to do. "],["organizing-your-project.html", "Chapter 10 Organizing your project 10.1 Learning Objectives 10.2 Organizational strategies 10.3 Readings about organizational strategies for data science projects: 10.4 Get the exercise project files (or continue with the files you used in the previous chapter) 10.5 Exercise: Organize your project!", " Chapter 10 Organizing your project 10.1 Learning Objectives Keeping your files organized is a skill that has a high long-term payoff. During the initial development of an analysis, you may underestimate how many files and terms you have floating around. But a short time later, you may return to your files and realize your organization was not as clear as you hoped. Tayo (2019) discusses four particular reasons why it is important to organize your project: Organization increases productivity. If a project is well organized, with everything placed in one directory, it makes it easier to avoid wasting time searching for project files such as datasets, codes, output files, and so on. A well-organized project helps you to keep and maintain a record of your ongoing and completed data science projects. Completed data science projects could be used for building future models. If you have to solve a similar problem in the future, you can use the same code with slight modifications. A well-organized project can easily be understood by other [researchers] when shared on platforms such as Github. Organization is yet another aspect of reproducibility that saves you and your colleagues time! 10.2 Organizational strategies There’s a lot of ways to keep your files organized, and there’s not a “one size fits all” organizational solution (Shapiro et al. 2021). In this chapter, we will discuss some generalities but as far as specifics we will point you to others who have written about works for them and advise that you use them as inspiration to figure out a strategy that works for you and your team. The most important aspects of your project organization scheme are that it: Is project-oriented (Bryan 2017). Follows consistent patterns (Shapiro et al. 2021). Is easy for you and others to find the files you need quickly (Shapiro et al. 2021). Minimizes the likelihood for errors (like writing over files accidentally) (Shapiro et al. 2021). Is something maintainable (Shapiro et al. 2021)! 10.2.1 Tips for organizing your project: Getting more specific, here’s some ideas of how to organize your project: Make file names informative to those who don’t have knowledge of the project. Avoid using spaces, quotes, or unusual characters in your filenames and folders – these only serve to make reading in files a nightmare in some programs. Number scripts/experiments in the order that they are run. Keep like-files together in their own directory: results tables with other results tables, etc. Including most importantly keeping raw data separate from processed data or other results! Put raw data, processed data, scripts and functions in their own directories. Be sure to leave raw data and other source information alone, as they should never need to be changed directly by yourself or anyone else. Put output in its own directories like results and plots. Have a central document (like a README) that describes the basic information about the analysis and how to re-run it. Make a central script that re-runs everything – including the creation of the folders! (more on this in a later chapter) Let’s see what these principles might look like put into practice. 10.2.1.1 Example organizational scheme Here’s an example of what this might look like: project-name/ ├── run_analysis.sh ├── 00-download-data.sh ├── 01-make-heatmap.Rmd ├── README.md ├── plots/ │ └── project-name-heatmap.png ├── results/ │ └── top_gene_results.tsv ├── raw-data/ │ ├── project-name-raw.tsv │ └── project-name-metadata.tsv ├── processed-data/ │ ├── project-name-quantile-normalized.tsv └── util/ ├── plotting-functions.R └── data-wrangling-functions.R What these hypothetical files and folders contain: run_analysis.sh - A central script that runs everything again 00-download-data.sh - The script that needs to be run first and is called by run_analysis.sh 01-make-heatmap.Rmd - The script that needs to be run second and is also called by run_analysis.sh README.md - The document that has the information that will orient someone to this project, we’ll discuss more about how to create a helpful README in an upcoming chapter. plots - A folder of plots and resulting images results - A folder of results raw-data - Data files as they first arrive and nothing has been done to them yet. processed-data - Data that have been modified from the raw in some way. util - A folder of utilities that never needs to be called or touched directly unless troubleshooting something 10.3 Readings about organizational strategies for data science projects: But you don’t have to take my organizational strategy, there are lots of ideas out there. You can read through some of these articles to think about what kind of organizational strategy might work for you and your team: Jenny Bryan’s organizational strategies (Bryan and Hester 2021). Danielle Navarro’s organizational strategies Navarro (2021) Jenny Bryan on Project-oriented workflows(Bryan 2017). Data Carpentry mini-course about organizing projects (“Project Organization and Management for Genomics” 2021). Andrew Severin’s strategy for organization (Severin 2021). A BioStars thread where many individuals share their own organizational strategies (“How Do You Manage Your Files &amp; Directories for Your Projects?” 2010). Data Carpentry course chapter about getting organized (“Introduction to the Command Line for Genomics” 2019). 10.4 Get the exercise project files (or continue with the files you used in the previous chapter) Get the Python project example files Click this link to download. Now double click your chapter zip file to unzip. For Windows you may have to follow these instructions. Get the R project example files Click this link to download. Now double click your chapter zip file to unzip. For Windows you may have to follow these instructions. 10.5 Exercise: Organize your project! Using your computer’s GUI (drag, drop, and clicking), organize the files that are part of this project. Organized these files using an organizational scheme similar to what is described above. Create folders like plots, results, and data folder. Note that aggregated_metadata.json and LICENSE.TXT also belong in the data folder. You will want to delete any files that say “OLD”. Keeping multiple versions of your scripts around is a recipe for mistakes and confusion. In the advanced course we will discuss how to use version control to help you track this more elegantly. Any feedback you have regarding this exercise is greatly appreciated; you can fill out this form! References "],["record-keeping-practices.html", "Chapter 11 Record keeping practices", " Chapter 11 Record keeping practices Once you have your project rolling, it is important to keep good records of your work, your collaborators work, and your communication. Keeping good records takes time and discipline but it can save you more time and heartache in the end. Here are some suggestions for how to optimize your record keeping. 11.0.1 Keep organized records of work Record and communicate notes about your data collection and analyses. Be mindful of overwhelming your coworkers, but generally speaking provide extra information where possible. The more people that are aware of details about what samples were in what batch, the more likely important details are not missed or forgotten. For example, if you are sending data to a collaborator, send as much information as possible about how it was generated in the email in which you send it to them, even if you have already discussed the data. This can help ensure that no important details fall through the cracks. The best way we think you can do this in general is to use reports - one of our next suggestions. 11.0.2 Keep organized records of communication Besides recording your work, keep a record of your communications. At a minimum organize your emails for projects into separate folders with easily recognizable titles to save yourself hassle later when something comes into question. However, we highly recommend that in addition for even better record keeping, you can use a note-taking system. This could be as simple as a shared Google doc, or you could consider an app like these that are designed for note-taking. With many of these you can also share your notes with research teammates and you can include report documents directly in your notes. Which brings us to our next point about using reports! 11.0.3 Use reports Instead of sending informal short emails (which are useful at some points in a workflow), we suggest intermittently sending lab reports with as much information about what was done and why. For informatics related work in R or Python (or other supported languages) we highly suggest using a method like R markdown or Jupyter notebooks to track what informatics steps you have performed and why. Beginning these reports with a short description of what raw data you used and when you received it can be critical for ensuring that you are using the correct data! We will describe more about how to use such reports in the final chapter of this course. It is also important that the experimental biologists make similar reports defining what reagents they used, when they performed the study, what samples were used, who performed the experiment, and any notes about unusual events, such as the electricity went out during the experiment, left the samples overnight but usually leave two hours, mouse #blank unexpectedly died so we lost this sample thus it is not included, or the dye seemed unusually faint in this gel. In summary, we recommend the following record keeping tips: "],["guidelines-for-good-metadata.html", "Chapter 12 Guidelines for Good Metadata 12.1 What are metadata? 12.2 How to create metadata?", " Chapter 12 Guidelines for Good Metadata 12.1 What are metadata? Metadata are critically important descriptive information about your data. Without metadata, the data themselves are useless or at best vastly limited. Metadata describe how your data came to be, what organism or patient the data are from and include any and every relevant piece of information about the samples in your data set. Metadata includes but isn’t limited to, the following example categories: At this time it’s important to note that if you work with human data or samples, your metadata will likely contain personal identifiable information (PII) and protected health information (PHI). It’s critical that you protect this information! For more details on this, we encourage you to see our course about data management. 12.2 How to create metadata? Where do these metadata come from? The notes and experimental design from anyone who played a part in collecting or processing the data and its original samples. If this includes you (meaning you have collected data and need to create metadata) let’s discuss how metadata can be made in the most useful and reproducible manner. 12.2.1 The goals in creating your metadata: 12.2.1.1 Goal A: Make it crystal clear and easily readable by both humans and computers! Some examples of how to make your data crystal clear: - Look out for typos and spelling errors! - Don’t use acronyms unless you need to and then if you do need to make sure to explain what the acronym means. - Don’t add extraneous information – perhaps items that are relevant to your lab internally but not meaningful to people outside of your lab. Either explain the significance of such information or leave it out. Make your data tidy. Tidy data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In tidy data: - Every column is a variable. - Every row is an observation. - Every cell is a single value. 12.2.1.2 Goal B: Avoid introducing errors into your metadata in the future! Toward these two goals, this excellent article by Broman &amp; Woo discusses metadata design rules. We will very briefly cover the major points here but highly suggest you read the original article. Be Consistent - Whatever labels and systems you choose, use it universally. This not only means in your metadata spreadsheet but also anywhere you are discussing your metadata variables. Choose good names for things - avoid spaces, special characters, or within the lab jargon. Write Dates as YYYY-MM-DD - this is a global standard and less likely to be messed up by Microsoft Excel. No Empty Cells - If a particular field is not applicable to a sample, you can put NA but empty cells can lead to formatting errors or just general confusion. Put Just One Thing in a Cell - resist the urge to combine variables into one, you have no limit on the number of metadata variables you can make! Make it a Rectangle - This is the easiest way to read data, for a computer and a human. Have your samples be the rows and variables be columns. Create a Data Dictionary - Have somewhere that you describe what your metadata mean in detailed paragraphs. No Calculations in the Raw Data Files - To avoid mishaps, you should always keep a clean, original, raw version of your metadata that you do not add extra calculations or notes to. Do Not Use Font Color or Highlighting as Data - This only adds to confusion of others if they don’t understand your color coding scheme. Instead create a new variable for anything you might be tempted to color code. Make Backups - Metadata are critical, you never want to lose them because of spilled coffee on a computer. Keep the original backed up in a multiple places - if your institution has a high performance server or cloud back up, we recommend using these solutions. See this course about computing for more information on these and other solutions. Use Data Validation to Avoid Errors - use spreadsheet software to check that the data in the columns is the type of data expected for a given variable. Note that it is very dangerous to open gene data with Excel. According to (Ziemann2016?), approximately one-fifth of papers with Excel gene lists have errors. This happens because Excel wants to interpret everything as a date. We strongly caution against opening (and saving afterward) gene data in Excel. 12.2.2 To recap: If you are not the person who has the information needed to create metadata, or you believe that another individual already has this information, make sure you get acquire the metadata that corresponding to your data. It will be critical for you to have in order to do any meaningful analysis! "],["data-storage-and-data-repositories.html", "Chapter 13 Data Storage and Data Repositories 13.1 Data Storage Concerns 13.2 Data Repositories 13.3 Considerations for Human Data 13.4 No Existing Repository? 13.5 Data repositories 13.6 Summary of resources: Finding a repository", " Chapter 13 Data Storage and Data Repositories To enable a more productive research experience and to ease your future compliance with any data sharing requirements, active data management during the research process can be very helpful. 13.1 Data Storage Concerns How should you store and interact with data while doing research? Your data storage needs will depend greatly on the type of data you will be working with as well as the number of samples you might have. File sizes vary considerably based on the data being stored. A single file can be as small as 1 MB in size (for a PET scan image of the heart) to 60 GB (for an uncompressed fastq of a whole genome sequence). Storing files in a compressed format, especially raw files, can help decrease your storage needs and costs. Below is a table of common types of data and sizes for single files. This list is not comprehensive but instead should be taken as a general guide. You should always get a more specific estimate for your particular project before submitting your grant proposal. Type of data Common size for a single file Genomics (WGS, WES) 15-60 GB Genomics (RNA-seq, scRNA-seq) 3-25 GB Imaging (microscopy) 2-8 MB Imaging (human medical) 1 MB – 2.5 GB Flow cytometry 1-50 MB Proteomics 3-5 MB Clinical trials 2-25 MB As you can see some of these data types require large data files. These files may quickly add up to require more storage or computing capacity than your laptop (which typically have 250 GB-1 TB of storage)! If you want to learn more about data file sizes check out the data file size details section of the appendix and this class on Computing for Cancer Informatics from the Informatics Technology for Cancer Research (ITCR) Training Network (ITN) for options on how to manage large data files. 13.2 Data Repositories Where will I share my data? What repositories exist that might work for my data type? Some programs or Funding Opportunity Announcements (FOA) will specify where the data should be shared. If this applies, you should plan to use the repositories mentioned in the FOA. Other programs or FOAs will not specify where the data should be shared, however the NIH provides an interactive table of NIH-supported data repositories to help you identify repositories that might be appropriate for your data. If you don’t find a repository there, additional repositories can also be found at the following links: Open NIH-supported domain-specific repositories Other NIH-supported domain-specific resources Nature data sharing resources Registry of research data repositories Researchers should aim to find a repository with the following characteristics according to the NIH: Established: If the repository is established (well-known or has been around for a significant period of time), it is likely to improve the FAIRness (Wilkinson et al. 2016) of the data. Specific: Repositories that are discipline or data-type specific should be prioritized to promote reuse. Unique Persistent Identifiers: Assigns datasets a citable, unique persistent identifier, such as a digital object identifier (DOI) or accession number, to support data discovery, reporting, and research assessment. The identifier points to a persistent landing page that remains accessible even if the dataset is de-accessioned or no longer available. Long-Term Sustainability: Has a plan for long-term management of data, including maintaining integrity, authenticity, and availability of datasets; building on a stable technical infrastructure and funding plans; and having contingency plans to ensure data are available and maintained during and after unforeseen events. Metadata: Ensures datasets are accompanied by metadata to enable discovery, reuse, and citation of datasets, using schema that are appropriate to, and ideally widely used across, the community/communities the repository serves. Domain-specific repositories would generally have more detailed metadata than generalist repositories. Curation and Quality Assurance: Provides, or has a mechanism for others to provide, expert curation and quality assurance to improve the accuracy and integrity of datasets and metadata. Free and Easy Access: Provides broad, equitable, and maximally open access to datasets and their metadata free of charge in a timely manner after submission, consistent with legal and ethical limits required to maintain privacy and confidentiality, Tribal sovereignty, and protection of other sensitive data. Broad and Measured Reuse: Makes datasets and their metadata available with broadest possible terms of reuse; and provides the ability to measure attribution, citation, and reuse of data (i.e., through assignment of adequate metadata and unique identifiers). Clear Use Guidance: Provides accompanying documentation describing terms of dataset access and use (e.g., particular licenses, need for approval by a data use committee). Security and Integrity: Has documented measures in place to meet generally accepted criteria for preventing unauthorized access to, modification of, or release of data, with levels of security that are appropriate to the sensitivity of data. Confidentiality: Has documented capabilities for ensuring that administrative, technical, and physical safeguards are employed to comply with applicable confidentiality, risk management, and continuous monitoring requirements for sensitive data (should your data require such safeguards). Common Format: Allows datasets and metadata downloaded, accessed, or exported from the repository to be in widely used, preferably non-proprietary, formats consistent with those used in the community/communities the repository serves. Provenance: Has mechanisms in place to record the origin, chain of custody, and any modifications to submitted datasets and metadata. Retention Policy: Provides documentation on policies for data retention within the repository. 13.3 Considerations for Human Data When working with human participant data, including de-identified human data, here are some additional characteristics to look for: Fidelity to Consent: Uses documented procedures to restrict dataset access and use to those that are consistent with participant consent and changes in consent. Restricted Use Compliant: Uses documented procedures to communicate and enforce data use restrictions, such as preventing reidentification or redistribution to unauthorized users. Privacy: Implements and provides documentation of measures (for example, tiered access, credentialing of data users, security safeguards against potential breaches) to protect human subjects data from inappropriate access. Plan for Breach: Has security measures that include a response plan for detected data breaches. Download Control: Controls and audits access to and download of datasets (if download is permitted). Violations: Has procedures for addressing violations of terms-of-use by users and data mismanagement by the repository. Request Review: Makes use of an established and transparent process for reviewing data access requests. Data FAIRness Data that is Findable, Accessible, Interoperable, and Reusable. For more information about data FAIRness, check out this manuscript by Wilkinson et al. (2016). In brief, it is described by the NIH as follows: To be Findable, data must have unique identifiers, effectively labeling it within searchable resources. To be Accessible, data must be easily retrievable via open systems and effective and secure authentication and authorization procedures. To be Interoperable, data should “use and speak the same language” via use of standardized vocabularies. To be Reusable, data must be adequately described to a new user, have clear information about data-usage licenses, and have a traceable “owner’s manual,” or provenance. 13.4 No Existing Repository? What if I can’t find an appropriate repository? Supplemental material - If the data are small (less than 2 GB), it may be included as supplemental material for an article. See here for more information. Institutional repositories - Check to see if your institute has a repository where you could publicly share the data Generalist repositories- Host your data somewhere that hosts different types of data publicly, such as: Dataverse Dryad Figshare IEEE Dataport Mendeley Data Open Science Framework Synapse Vivli Zenodo Note that the NIH encourages that an existing data sharing repository be used whenever one is available instead of one of these options. 13.5 Data repositories The best way to share your data is by putting it somewhere that others can download it (and it can be kept private when necessary). There’s many repositories out there that handle this for you. Below are some of the standard repositories for data you should consider. For a longer list of repositories, we also advise consulting this Nature guidance on data repositories. 13.5.1 Genomic Data Repositories National Center for Biotechnology Information (NCBI) For microarray: GEO Gene Expression Omnibus (GEO) For RNA-seq: SRA (Sequencing Read Archive) European Molecular Biology Laboratory-European Bioinformatics Institute (EMBL-EBI) International Nucleotide Sequence Databases—DNA Data Bank of Japan (DDBJ) 13.5.2 Imaging data repositories Imaging data resource Cancer imaging archive 13.5.3 Repositories for journal articles For manuscripts or large datasets that are of atypical format, using one of these repositories is a good idea. The journal you submit to may have a recommendation of one over another. If not, you might end up having a preference. CyVerse Data Commons Repository Data Dryad FigShare ZENODO 13.5.4 Small datasets Data sets that are small and atypical in format can be published as supplementary files as a part of a manuscript. 13.6 Summary of resources: Finding a repository The following links can help you find a data repository for your data: Interactive table of NIH-supported data repositories Start here! Open NIH-supported domain-specific repositories Other NIH-supported domain-specific resources Nature data sharing resources Registry of research data repositories If you don’t find an appropriate repository for your data type: Consider adding your data as a supplementary file to a manuscript if it is small Consider an institutional repository Check out the generalist repositories References "],["health-care-data-sharing-tools.html", "Chapter 14 Health care data sharing tools", " Chapter 14 Health care data sharing tools 14.0.1 REDCap (Research Electronic Data Capture) REDCap is a very widely used browser-based software application for managing surveys and databases. It is very often used for clinical data. In fact, it is so widely used that there is a conference dedicated to it. REDCap allows for multi-institutional work, as well as compliance with HIPAA, 21 CFR Part 11 for data for the FDA, FISMA for government data, HIPAA, and GDPR for data for the European Union. It was developed by a team at Vanderbilt University in 2004. It is not open-source, however it is free to use for non-commercial research (redcap_2022?). You can find out more about how to use REDCap at the REDCap website which includes instructional videos and other resources. There are several things to keep in mind when using REDCap from an ethical standpoint. Roles REDCap allows for various roles to be established for users on a project. Thus access to certain data and tasks can be restricted to certain individuals. As described previously, it is a good idea to restrict access to the smallest number of individuals necessary. You can modify these roles using the User Rights menu. This will first show you who has what role on the project and their rights. You can click on an individual role to modify it. These roles should be verified by your institutional review board (IRB) before beginning a study. Changes to roles should also be reviewed by your IRB. Reports Reports that are exported can be customized to only show data that should be shared with the individual that you plan to share with. Please see the section on de-identification to better understand what data you might want to be restrictive about sharing. Again, the way you intend to share your data should be reviewed by your IRB before you begin your study. For example, you might remove the dates from the following report: Auditing REDCap keeps track of all data modifications, as well as data exports or report generations, in addition to keeping track of who performs those actions. This can be helpful for checking what has happened and when, in case anything happens that is unexpected or unintended. This is also great from a reproducibility or transparency standpoint - you have a record of any modifications to the data. This information can be obtained from the logging menu. Keep instruments short If your instruments are too long, this can result in accidentally sharing data that you don’t intend to, simply because you have more data to sift through. This also makes it easier to generate reports only on specific data that you would like to share. Data can be locked You can protect your data from accidentally being modified by locking specific data. Furthermore, at later stages of the project the data can no longer be modified. Keep in mind that your institution likely has their own guidelines for how to use REDCap should you decide to use it. Also remember to verify what you plan to do with your institutional review board (IRB) before you begin the study. "],["about-the-authors.html", "About the Authors", " About the Authors These credits are based on our course contributors table guidelines.     Credits Names Pedagogy Lead Content Instructor(s) Carrie Wright Candace Savonen Content Editor(s)/Reviewer(s) Nathan Boyd Original Authors This course uses content from other courses authored by Carrie Wright Candace Savonen, Ava Hoffman Technical Course Publishing Engineer(s) Candace Savonen Template Publishing Engineers Candace Savonen, Carrie Wright, Ava Hoffman Publishing Maintenance Engineer Candace Savonen Technical Publishing Stylists Carrie Wright, Ava Hoffman, Candace Savonen Package Developers (ottrpal) Candace Savonen, John Muschelli, Carrie Wright Art and Design Illustrator(s) Carrie Wright Candace Savonen Figure Artist(s) Carrie Wright Candace Savonen Funding Funder(s) NCI UE5CA254170 Funding Staff [Shasta Nicholson], Maleah O’Connor, [Sandy Ombrek]   ## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 4.3.2 (2023-10-31) ## os Ubuntu 22.04.4 LTS ## system x86_64, linux-gnu ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz Etc/UTC ## date 2025-10-06 ## pandoc 3.1.1 @ /usr/local/bin/ (via rmarkdown) ## ## ─ Packages ─────────────────────────────────────────────────────────────────── ## package * version date (UTC) lib source ## bookdown 0.43 2025-04-15 [1] CRAN (R 4.3.2) ## bslib 0.6.1 2023-11-28 [1] RSPM (R 4.3.0) ## cachem 1.0.8 2023-05-01 [1] RSPM (R 4.3.0) ## chromote 0.5.1 2025-04-24 [1] CRAN (R 4.3.2) ## cli 3.6.2 2023-12-11 [1] RSPM (R 4.3.0) ## devtools 2.4.5 2022-10-11 [1] RSPM (R 4.3.0) ## digest 0.6.34 2024-01-11 [1] RSPM (R 4.3.0) ## dplyr 1.1.4 2023-11-17 [1] RSPM (R 4.3.0) ## ellipsis 0.3.2 2021-04-29 [1] RSPM (R 4.3.0) ## evaluate 1.0.4 2025-06-18 [1] CRAN (R 4.3.2) ## fansi 1.0.6 2023-12-08 [1] RSPM (R 4.3.0) ## fastmap 1.1.1 2023-02-24 [1] RSPM (R 4.3.0) ## fs 1.6.3 2023-07-20 [1] RSPM (R 4.3.0) ## generics 0.1.3 2022-07-05 [1] RSPM (R 4.3.0) ## gitcreds 0.1.2 2022-09-08 [1] RSPM (R 4.3.0) ## glue 1.7.0 2024-01-09 [1] RSPM (R 4.3.0) ## hms 1.1.3 2023-03-21 [1] RSPM (R 4.3.0) ## htmltools 0.5.7 2023-11-03 [1] RSPM (R 4.3.0) ## htmlwidgets 1.6.4 2023-12-06 [1] RSPM (R 4.3.0) ## httpuv 1.6.14 2024-01-26 [1] RSPM (R 4.3.0) ## httr 1.4.7 2023-08-15 [1] RSPM (R 4.3.0) ## jquerylib 0.1.4 2021-04-26 [1] RSPM (R 4.3.0) ## jsonlite 1.8.8 2023-12-04 [1] RSPM (R 4.3.0) ## knitr 1.50 2025-03-16 [1] CRAN (R 4.3.2) ## later 1.3.2 2023-12-06 [1] RSPM (R 4.3.0) ## lifecycle 1.0.4 2023-11-07 [1] RSPM (R 4.3.0) ## magrittr 2.0.3 2022-03-30 [1] RSPM (R 4.3.0) ## memoise 2.0.1 2021-11-26 [1] RSPM (R 4.3.0) ## mime 0.12 2021-09-28 [1] RSPM (R 4.3.0) ## miniUI 0.1.1.1 2018-05-18 [1] RSPM (R 4.3.0) ## ottrpal 2.0.0 2025-08-11 [1] Github (ottrproject/ottrpal@a9049b7) ## pillar 1.9.0 2023-03-22 [1] RSPM (R 4.3.0) ## pkgbuild 1.4.3 2023-12-10 [1] RSPM (R 4.3.0) ## pkgconfig 2.0.3 2019-09-22 [1] RSPM (R 4.3.0) ## pkgload 1.3.4 2024-01-16 [1] RSPM (R 4.3.0) ## processx 3.8.3 2023-12-10 [1] RSPM (R 4.3.0) ## profvis 0.3.8 2023-05-02 [1] RSPM (R 4.3.0) ## promises 1.2.1 2023-08-10 [1] RSPM (R 4.3.0) ## ps 1.7.6 2024-01-18 [1] RSPM (R 4.3.0) ## purrr 1.0.2 2023-08-10 [1] RSPM (R 4.3.0) ## R6 2.5.1 2021-08-19 [1] RSPM (R 4.3.0) ## Rcpp 1.0.12 2024-01-09 [1] RSPM (R 4.3.0) ## readr 2.1.5 2024-01-10 [1] RSPM (R 4.3.0) ## remotes 2.4.2.1 2023-07-18 [1] RSPM (R 4.3.0) ## rlang 1.1.6 2025-04-11 [1] CRAN (R 4.3.2) ## rmarkdown 2.25 2023-09-18 [1] RSPM (R 4.3.0) ## rprojroot 2.1.0 2025-07-12 [1] CRAN (R 4.3.2) ## rvest 1.0.4 2024-02-12 [1] CRAN (R 4.3.2) ## sass 0.4.8 2023-12-06 [1] RSPM (R 4.3.0) ## sessioninfo 1.2.2 2021-12-06 [1] RSPM (R 4.3.0) ## shiny 1.8.0 2023-11-17 [1] RSPM (R 4.3.0) ## spelling 2.3.1 2024-10-04 [1] CRAN (R 4.3.2) ## stringi 1.8.3 2023-12-11 [1] RSPM (R 4.3.0) ## stringr 1.5.1 2023-11-14 [1] RSPM (R 4.3.0) ## tibble 3.3.0 2025-06-08 [1] CRAN (R 4.3.2) ## tidyr 1.3.1 2024-01-24 [1] RSPM (R 4.3.0) ## tidyselect 1.2.0 2022-10-10 [1] RSPM (R 4.3.0) ## tzdb 0.4.0 2023-05-12 [1] RSPM (R 4.3.0) ## urlchecker 1.0.1 2021-11-30 [1] RSPM (R 4.3.0) ## usethis 2.2.3 2024-02-19 [1] RSPM (R 4.3.0) ## utf8 1.2.4 2023-10-22 [1] RSPM (R 4.3.0) ## vctrs 0.6.5 2023-12-01 [1] RSPM (R 4.3.0) ## webshot2 0.1.2 2025-04-23 [1] CRAN (R 4.3.2) ## websocket 1.4.4 2025-04-10 [1] CRAN (R 4.3.2) ## xfun 0.52 2025-04-02 [1] CRAN (R 4.3.2) ## xml2 1.3.6 2023-12-04 [1] RSPM (R 4.3.0) ## xtable 1.8-4 2019-04-21 [1] RSPM (R 4.3.0) ## yaml 2.3.10 2024-07-26 [1] CRAN (R 4.3.2) ## ## [1] /usr/local/lib/R/site-library ## [2] /usr/local/lib/R/library ## ## ────────────────────────────────────────────────────────────────────────────── "],["references.html", "Chapter 15 References", " Chapter 15 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
